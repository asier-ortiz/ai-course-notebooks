{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Mejoras para este ejercicio:\n",
    "# - Asegurar que los datos sean consistentes: si hay valores erróneos o nulos, establecer un random_state para garantizar reproducibilidad\n",
    "# - Implementar pipelines para estructurar mejor el flujo de preprocesamiento y modelado, evitando la aplicación manual de cada paso\n",
    "# - Optimizar el modelo ajustando hiperparámetros con técnicas como GridSearchCV o RandomizedSearchCV\n",
    "# - Utilizar Regex para validaciones: códigos postales, teléfonos, emails, etc.\n",
    "# - Crear variables derivadas como precio por metro cuadrado (precio_m2 = precio / superficie)\n",
    "# - Geolocalización: obtener coordenadas con OpenStreetMap a partir de direcciones o códigos postales y utilizarlas para análisis espaciales\n",
    "# - Visualizar las viviendas en un mapa interactivo con Folium o Plotly Express para identificar patrones geográficos en los precios\n",
    "# - Clusterización de zonas con K-Means o DBSCAN para detectar patrones de precios por ubicación y segmentar mejor los inmuebles\n",
    "# - Evitar data leakage: Dividir los datos en train/test antes de hacer encoding, eliminar outliers o escalar,\n",
    "#   asegurando que las transformaciones se ajusten sólo con el conjunto de entrenamiento y luego se apliquen en test\n",
    "# - Subir el proyecto final a Kaggle"
   ],
   "id": "be40bb5aac5b5e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Funciones Helper**\n",
    "Aquí se encuentran las funciones auxiliares utilizadas en este notebook. Estas funciones permiten realizar tareas específicas de manera más ordenada y modular."
   ],
   "id": "dedb5826bbe4b858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_floor_column(df):\n",
    "    \"\"\"\n",
    "    Limpia la columna 'floor' asegurando consistencia en los valores.\n",
    "    - Convierte valores ordinales como '1st', '2nd', '3rd' en números.\n",
    "    - Sustituye el valor 'ground' por 0.\n",
    "    - Convierte en NaN otros valores como 'floor'.\n",
    "    - Maneja valores extremadamente altos o negativos.\n",
    "    - Convierte a tipo Int16 para optimizar memoria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mostrar valores únicos antes del saneamiento\n",
    "    print(\"\\nValores únicos en 'floor' antes del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    # Diccionario de conversión de valores ordinales (1st, 2nd, etc.)\n",
    "    ordinal_mapping = {f\"{i}th\": i for i in range(1, 101)}\n",
    "    ordinal_mapping.update({\"ground\": 0, \"1st\": 1, \"2nd\": 2, \"3rd\": 3})\n",
    "\n",
    "    # Convertir valores de la columna\n",
    "    cleaned_floors = []\n",
    "    for value in df[\"floor\"]:\n",
    "        if pd.isna(value):\n",
    "            cleaned_floors.append(pd.NA)  # Mantener valores nulos\n",
    "            continue\n",
    "\n",
    "        value = str(value).lower().strip()\n",
    "\n",
    "        # Intentar convertir directamente si es un número\n",
    "        try:\n",
    "            num_value = int(value.replace(\",\", \"\"))  # Manejo de valores con comas como \"1,200\"\n",
    "            if 0 <= num_value <= 100:  # Limitar a un rango razonable de pisos\n",
    "                cleaned_floors.append(num_value)\n",
    "            else:\n",
    "                cleaned_floors.append(pd.NA)  # Si el número es absurdo, lo dejamos como NA\n",
    "        except ValueError:\n",
    "            cleaned_floors.append(ordinal_mapping.get(value, pd.NA))  # Mapear valores ordinales o NA si no se reconoce\n",
    "\n",
    "    # Asignar la columna limpia al DataFrame\n",
    "    df[\"floor\"] = cleaned_floors\n",
    "\n",
    "    # Convertir a Int16 para optimizar memoria, manteniendo NaN con pd.NA\n",
    "    df[\"floor\"] = df[\"floor\"].astype(\"Int16\") # TODO Hacer esto en la conversión\n",
    "\n",
    "    # Mostrar valores únicos después del saneamiento\n",
    "    print(\"\\nValores únicos en 'floor' después del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def is_within_madrid(lat, lng):\n",
    "    \"\"\"\n",
    "    Verifica si una ubicación dada por latitud y longitud está dentro de un radio determinado\n",
    "    desde el centro de Madrid.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lat: float\n",
    "        Latitud de la ubicación a verificar.\n",
    "    lng: float\n",
    "        Longitud de la ubicación a verificar.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    bool\n",
    "        True si la ubicación está dentro del radio permitido, False en caso contrario.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Se toma como referencia el centro de Madrid con coordenadas (40.4168, -3.7038).\n",
    "    - Se define un umbral de distancia máxima de 60 km para considerar una ubicación válida.\n",
    "    - Se usa la función `geodesic` de geopy para calcular la distancia en kilómetros.\n",
    "    - Si la latitud o longitud son nulas, la función retorna False para evitar errores.\n",
    "    \"\"\"\n",
    "\n",
    "    madrid_center = (40.4168, -3.7038)\n",
    "    radius_km = 60\n",
    "\n",
    "    if pd.isna(lat) or pd.isna(lng):\n",
    "        return False\n",
    "\n",
    "    distance = geodesic((lat, lng), madrid_center).km  # Calcula la distancia al centro de Madrid\n",
    "    return distance <= radius_km  # Devuelve True si está dentro del radio, False si está fuera"
   ],
   "id": "d3321250ab1bbe27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 - Carga de datos y revisión de la estructura del dataset\n",
    "\n",
    "En este apartado se realiza una **primera exploración del dataset** para comprender su estructura, tipo de variables y posibles relaciones entre ellas. Este paso es fundamental para la correcta preparación de los datos antes del modelado."
   ],
   "id": "f207b07e6af43aa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.1 - Importación de librerías y configuración**\n",
    "Se importan las librerías esenciales para el análisis de datos, la visualización y el modelado, incluyendo **Pandas, NumPy, Matplotlib, Seaborn, SciPy y Scikit-Learn**. Además, se configuran algunos parámetros globales de visualización para mejorar la legibilidad de los gráficos."
   ],
   "id": "68947ca93b1ee80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Configuraciones\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)"
   ],
   "id": "cae123131f2ff1e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.2 - Carga del dataset**\n",
    "Se carga el conjunto de datos en un **DataFrame de Pandas** desde un archivo CSV. Se incluye una referencia a la fuente del dataset."
   ],
   "id": "642fbdc3166ffe7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar el dataset local con Pandas\n",
    "df = pd.read_csv(\"scripts/madrid_rent_with_coordinates.csv\")\n",
    "\n",
    "# Ref. https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data"
   ],
   "id": "57f78ce23e88ed17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.3 - Inspección de la estructura del dataset**\n",
    "En esta fase se inspecciona la estructura general del dataset para entender su contenido y formato:\n",
    "\n",
    "- **Visualización de las primeras y últimas filas** para detectar posibles errores en la carga de los datos.\n",
    "- **Información general del dataset** (`df.info()`), que muestra el número de registros, tipos de datos y valores nulos.\n",
    "- **Número total de filas y columnas** (`df.shape`).\n",
    "- **Identificación de columnas numéricas y categóricas**, que ayudará en el preprocesamiento.\n",
    "- **Recuento de valores únicos en variables categóricas**, útil para evaluar su diversidad.\n",
    "- **Visualización de los valores únicos en algunas columnas categóricas clave**, como `type`, `floor`, `orientation`, `district` y `subdistrict`, con el objetivo de analizar su distribución y detectar posibles inconsistencias.\n",
    "- **Revisión del número de valores nulos por columna** (`df.isnull().sum()`), lo que permitirá identificar qué variables requieren imputación o eliminación en la etapa de preprocesamiento.\n",
    "- **Resumen estadístico de las variables numéricas**, para analizar su distribución, detectar valores atípicos y entender la escala de los datos."
   ],
   "id": "dc595c93d116a8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las primeras filas para una vista inicial del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())"
   ],
   "id": "a652c94054a75007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las últimas filas para identificar posibles problemas en la carga de datos\n",
    "print(\"Últimas filas del dataset:\")\n",
    "display(df.tail())"
   ],
   "id": "b29d30b31ad16871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Obtener información general sobre el dataset, incluyendo tipos de datos y valores nulos\n",
    "print(\"Información general del dataset:\")\n",
    "display(df.info())"
   ],
   "id": "cff1b45136ac14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el número de filas y columnas en el dataset\n",
    "print(\"Número de filas y columnas en el dataset:\")\n",
    "print(df.shape)"
   ],
   "id": "d72fee52d985c328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar las columnas numéricas y categóricas\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ],
   "id": "6cc0a5878406bc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas numéricas identificadas\n",
    "print(\"Columnas numéricas:\")\n",
    "print(numerical_columns)"
   ],
   "id": "b8e474b4b602fdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas categóricas identificadas\n",
    "print(\"Columnas categóricas:\")\n",
    "print(categorical_columns)"
   ],
   "id": "3c8f87e02e5478fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar valores únicos en variables categóricas\n",
    "print(\"Valores únicos en variables categóricas:\")\n",
    "df[categorical_columns].nunique()"
   ],
   "id": "1e7904d2ca243a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores únicos de algunas columnas categóricas para revisar su diversidad\n",
    "relevant_categorical_columns = ['type', 'floor', 'orientation', 'district', 'subdistrict']\n",
    "\n",
    "print(\"Valores únicos en algunas variables categóricas relevantes:\")\n",
    "for col in relevant_categorical_columns:\n",
    "    print(f\"\\n{col}: {df[col].nunique()} valores únicos\")\n",
    "    print(df[col].unique()[:10])"
   ],
   "id": "d0f9ab1f50b72cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el número de valores nulos por columna\n",
    "print(df.isnull().sum())"
   ],
   "id": "aeed5067783abd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Describir estadísticamente las variables numéricas para analizar su distribución y posibles valores atípicos\n",
    "print(\"Resumen estadístico de las variables numéricas:\")\n",
    "display(df.describe())"
   ],
   "id": "5f6b3ad50b6bc66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.4 - Comprobación de relaciones potenciales**\n",
    "Para evaluar la viabilidad del modelado, se analizan las correlaciones entre variables numéricas:\n",
    "\n",
    "- Se genera una **matriz de correlación** (`df.corr()`) para medir la relación entre variables.\n",
    "- Se identifican **las variables con mayor impacto en `price`** basándose en la correlación:\n",
    "  - `floor_built` (0.70)\n",
    "  - `floor_area` (0.72)\n",
    "  - `bedrooms` (0.51)\n",
    "  - `bathrooms` (0.69)\n",
    "- Se confirma la pertinencia de `balcony` como una **variable categórica de clasificación**, revisando la distribución de sus valores."
   ],
   "id": "3c939f1df9844c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se genera la matriz de correlación para analizar la relación entre las variables numéricas\n",
    "print(\"Matriz de correlación entre variables numéricas:\")\n",
    "correlation_matrix = df[numerical_columns].corr()\n",
    "display(correlation_matrix)"
   ],
   "id": "b88cd433b80c151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar relaciones relevantes para modelado\n",
    "\n",
    "# Basándonos en la matriz de correlación, seleccionamos las variables con mayor impacto en 'price'\n",
    "# Según el análisis previo, las variables con correlación más fuerte con 'price' son:\n",
    "# - floor_built (0.70)\n",
    "# - floor_area (0.72)\n",
    "# - bedrooms (0.51)\n",
    "# - bathrooms (0.69)\n",
    "# Otras variables tienen correlaciones insignificantes (<0.01) y no se consideran para modelado\n",
    "\n",
    "key_relationships = ['price', 'floor_built', 'floor_area', 'bedrooms', 'bathrooms']\n",
    "print(\"Relaciones clave para la predicción de price:\")\n",
    "display(df[key_relationships].corr())"
   ],
   "id": "7f431580b67734b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se selecciona balcony como variable de clasificación\n",
    "# Se analiza la frecuencia de los valores presentes en balcony para entender su distribución\n",
    "print(\"Frecuencia de valores en la variable balcony:\")\n",
    "df['balcony'].value_counts()"
   ],
   "id": "99008d5a2872faa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2 - Limpieza y validación de los datos**\n",
    "\n",
    "Este apartado se centra en la **depuración del dataset** para garantizar que los datos sean consistentes, relevantes y adecuados para el modelado. Se eliminan columnas innecesarias, se manejan valores duplicados y se corrigen inconsistencias en los valores."
   ],
   "id": "5908d74e2a81ac48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.1 - Eliminación de columnas irrelevantes**\n",
    "Se eliminan columnas que no aportan información útil para el análisis de precios y balcones. Estas incluyen identificadores únicos (`web_id`), enlaces (`url`), información redundante (`title`), y metadatos sin valor predictivo (`professional_name`, `last_update`, `location`)."
   ],
   "id": "ef1f12f8be859629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se eliminan columnas que no aportan información útil para el análisis de precios y balcones\n",
    "columns_to_drop = [\n",
    "    'web_id',               # Identificador único para cada anuncio, no aporta valor analítico\n",
    "    'url',                  # Enlace a la página del anuncio, irrelevante para el modelado\n",
    "    'title',                # Contiene información redundante, ya que 'type' y 'location' están en otras columnas\n",
    "    'professional_name',    # Nombre de la agencia inmobiliaria o propietario, sin impacto en la predicción\n",
    "    'last_update',          # Indica la fecha de última actualización del anuncio, pero sin un formato uniforme y sin valor predictivo\n",
    "    'location'              # Contiene el nombre de la calle y en algunos casos el número, pero ya se tiene 'district' y 'subdistrict'\n",
    "]\n",
    "\n",
    "# Eliminamos las columnas del dataframe\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostramos el nuevo número de columnas para verificar la eliminación\n",
    "print(\"Columnas eliminadas:\", columns_to_drop)\n",
    "print(\"Número de columnas tras la eliminación:\", df.shape[1])"
   ],
   "id": "f033ecda8ee99955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.2 - Manejo de duplicados**\n",
    "Se identifican y eliminan registros duplicados para evitar sesgos en el análisis. Se revisan duplicados completos y duplicados parciales en base a características clave como `price`, `floor_area` y `bedrooms`."
   ],
   "id": "989a9fc4e765552d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar filas duplicadas considerando todas las columnas\n",
    "duplicates_total = df.duplicated().sum()\n",
    "print(f\"Número de filas completamente duplicadas: {duplicates_total}\")"
   ],
   "id": "7d37cac7a17b1ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Seleccionar columnas clave para identificar duplicados parciales excluyendo IDs o metadatos\n",
    "#\n",
    "# Para detectar duplicados, se consideran las columnas que describen las características estructurales de la vivienda,\n",
    "# evitando aquellas que son únicas para cada anuncio o irrelevantes para el análisis.\n",
    "#\n",
    "# Columnas **NO INCLUIDAS** en la detección de duplicados y razones:\n",
    "# - `web_id` y `url`: Son identificadores únicos de cada anuncio, por lo que no sirven para detectar duplicados.\n",
    "# - `title`: Contiene información redundante sobre el tipo de vivienda y ubicación, pero en formato texto libre, lo que\n",
    "#   lo hace inconsistente para la comparación.\n",
    "# - `professional_name`: El nombre de la agencia o propietario no influye en si un anuncio es duplicado o no.\n",
    "# - `last_update`: Fecha de actualización del anuncio con valores inconsistentes (por ejemplo, \"2 months\" vs. \"5 November\"),\n",
    "#   lo que impide su uso para identificar duplicados.\n",
    "# - `location`: Contiene el nombre de la calle y en algunos casos el número, pero ya disponemos de `district` y `subdistrict`\n",
    "#   que son más estructurados. Además, las direcciones pueden estar escritas de forma diferente en anuncios duplicados.\n",
    "#\n",
    "# Se opta por considerar características clave de la vivienda como `price`, `bedrooms`, `floor_area`, `year_built`, etc.,\n",
    "# ya que un mismo inmueble puede aparecer varias veces con ligeras variaciones en el título, la inmobiliaria o la fecha de publicación.\n",
    "\n",
    "duplicate_columns = ['type', 'price', 'deposit', 'floor_built', 'floor_area',\n",
    "                     'year_built', 'bedrooms', 'bathrooms', 'second_hand',\n",
    "                     'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                     'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                     'storeroom', 'swimming_pool', 'garden_area', 'district', 'subdistrict',\n",
    "                     'postalcode', 'lat', 'lng']\n",
    "\n",
    "# Contar filas que tienen duplicación en estas columnas\n",
    "duplicates_partial = df.duplicated(subset=duplicate_columns).sum()\n",
    "print(f\"Número de filas duplicadas considerando solo columnas clave: {duplicates_partial}\")"
   ],
   "id": "97db98a7af945d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar filas completamente duplicadas\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset después de eliminar duplicados completos: {df.shape}\")"
   ],
   "id": "a935b676f343b279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar duplicados en base a columnas clave, manteniendo la primera aparición\n",
    "df.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)\n",
    "print(f\"Dataset después de eliminar duplicados parciales: {df.shape}\")"
   ],
   "id": "243823915c0f68e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.3 - Eliminación de columnas con alto porcentaje de valores nulos**\n",
    "Se eliminan variables con un gran número de valores nulos que no pueden imputarse de forma fiable, como `year_built`, `orientation` y `postalcode`. Se justifica la retención de otras variables con nulos si son clave para el análisis."
   ],
   "id": "581e0484cc64c14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "\n",
    "# Mostrar el porcentaje de nulos ordenado de mayor a menor\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage.sort_values(ascending=False))"
   ],
   "id": "92eef3b52f84fc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  floor_area (57%) → Muy relevante para el precio. Mantener y evaluar imputación aunque tenga un alto porcentaje de nulos\n",
    "#  deposit (41%) → No es muy relevante para el precio. Pese a ello, mantener y evaluar imputación aunque tenga un alto porcentaje de nulos\n",
    "\n",
    "# Definir las columnas a eliminar\n",
    "columns_to_drop = [\n",
    "    'year_built',      # Muchos nulos (68%). Aunque debería ser relevante para price (construcciones modernas podrían ser más caras) es difícil de imputar correctamente\n",
    "    'orientation',     # Muchos nulos (52%), difícil de imputar correctamente\n",
    "    'postalcode'       # Porcentaje de nulos moderadamente alto (25%). Redundante con district y subdistrict, no aporta información adicional\n",
    "]\n",
    "\n",
    "# Eliminar las columnas del DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostrar las columnas eliminadas y el nuevo número de columnas\n",
    "print(\"Columnas eliminadas por alto porcentaje de valores nulos:\", columns_to_drop)\n",
    "print(\"Número de columnas después de la eliminación:\", df.shape[1])"
   ],
   "id": "bab2a59024a96627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.4 - Saneamiento de valores en columnas categóricas**\n",
    "Se homogeneizan valores en columnas categóricas como `type`, `district` y `subdistrict`, eliminando inconsistencias en mayúsculas/minúsculas, espacios en blanco y valores incorrectos."
   ],
   "id": "1315567ee7068831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneización de la columna 'type' para asegurar consistencia en los valores\n",
    "print(\"Valores únicos en 'type' antes del saneamiento:\", df['type'].unique(), \"\\n\")\n",
    "\n",
    "# Elimino espacios en blanco y capitalizo los valores\n",
    "df['type'] = df['type'].str.strip().str.title()\n",
    "\n",
    "print(\"Valores únicos en 'type' después del saneamiento:\", df['type'].unique())"
   ],
   "id": "4447283eec9bb833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Limpieza a la columna 'floor' para estandarizar valores, corregir inconsistencias y pasar los datos a numérico mediante la función 'clear_floor_column'\n",
    "df = clean_floor_column(df)"
   ],
   "id": "dddc934de804cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneización básica de 'district' y 'subdistrict'\n",
    "\n",
    "# Estas columnas solo se utilizarán en el análisis exploratorio de datos (EDA),\n",
    "# por lo que aplicamos una limpieza mínima para garantizar consistencia:\n",
    "# - Convertimos los valores a minúsculas para evitar diferencias por mayúsculas/minúsculas.\n",
    "# - Eliminamos espacios en blanco adicionales para garantizar uniformidad.\n",
    "# - Reemplazamos valores 'nan' en formato string por valores NaN reales.\n",
    "# - No realizamos agrupaciones de nombres similares, ya que no afectarán al modelo.\n",
    "\n",
    "# Asegurar que sean de tipo 'object' antes de limpiar (evita problemas si son 'category')\n",
    "df['district'] = df['district'].astype(str).str.strip().str.lower()\n",
    "df['subdistrict'] = df['subdistrict'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Reemplazar valores 'nan' string por NaN reales de manera robusta\n",
    "df['district'] = df['district'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "df['subdistrict'] = df['subdistrict'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "\n",
    "# Mostramos los valores únicos después del saneamiento\n",
    "print(\"Valores únicos en 'district' después del saneamiento:\")\n",
    "display(df['district'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Valores únicos en 'subdistrict' después del saneamiento:\")\n",
    "display(df['subdistrict'].unique())"
   ],
   "id": "6e742c95b92a7ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.5 - Saneamiento de valores en columnas numéricas**\n",
    "Se sustituyen por `NaN` los valores negativos en columnas donde no pueden existir (e.g., `price`, `floor_area`, `bathrooms`). Además, se eliminan valores 0 en columnas donde no tienen sentido (`price`, `floor_area`, `floor_built`)."
   ],
   "id": "dcf7c9b60efd7137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores negativos donde no pueden existir\n",
    "not_neg_columns = ['price', 'deposit', 'floor_area', 'floor_built', 'floor', 'bedrooms', 'bathrooms']\n",
    "for col in not_neg_columns:\n",
    "    df.loc[df[col] < 0, col] = pd.NA"
   ],
   "id": "69b67e1fb540322b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores 0 en columnas donde no tiene sentido\n",
    "not_zero_columns = ['price', 'floor_area', 'floor_built']\n",
    "for col in not_zero_columns:\n",
    "    df.loc[df[col] == 0, col] = pd.NA"
   ],
   "id": "d3e48d4f9ccd58c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores negativos restantes\n",
    "for col in not_neg_columns:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    print(f\" {neg_count} valores negativos restantes en '{col}'.\")"
   ],
   "id": "d73d99ac208c5f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores 0 restantes en las columnas afectadas\n",
    "print(\"\\nValores 0 restantes en columnas afectadas:\")\n",
    "for col in not_zero_columns:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    print(f\" - {col}: {zero_count} valores con valor 0\")"
   ],
   "id": "b2794f8e35d43b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar cuántos valores nulos quedaron en cada columna numérica saneada\n",
    "print(\"\\nValores nulos después del saneamiento:\")\n",
    "print(df[not_neg_columns].isna().sum())"
   ],
   "id": "9b8cfd976a1e7767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.6 - Saneamiento de valores de geolocalización**\n",
    "\n",
    "El dataset incluye coordenadas de latitud (`lat`) y longitud (`lng`) obtenidas mediante la API de Google Maps. Sin embargo, algunos inmuebles pueden tener ubicaciones incorrectas fuera de la **Comunidad de Madrid**, posiblemente debido a errores en la geocodificación.\n",
    "\n",
    "Para corregir esto, se define una función que calcula la distancia entre cada propiedad y el centro de Madrid. Se establece un **radio máximo de 60 km**, eliminando aquellas viviendas que se encuentren fuera de este límite.\n",
    "\n",
    "Este proceso garantiza que los datos reflejen únicamente inmuebles dentro del área de interés y mejora la precisión del análisis."
   ],
   "id": "e592e46393afd6fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar los registros fuera de Madrid\n",
    "df[\"valid_location\"] = df.apply(lambda row: is_within_madrid(row[\"lat\"], row[\"lng\"]), axis=1)"
   ],
   "id": "e6fe23cf5c4d3b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar cuántos registros serán eliminados\n",
    "outliers_count = df[~df[\"valid_location\"]].shape[0]\n",
    "print(f\"Número de registros fuera de la Comunidad de Madrid a eliminar: {outliers_count}\")"
   ],
   "id": "c042f11964e5d594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar los registros fuera de Madrid\n",
    "df = df[df[\"valid_location\"]].drop(columns=[\"valid_location\"]).reset_index(drop=True)"
   ],
   "id": "9f60fd243e0e8878",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confirmar el nuevo tamaño del dataset\n",
    "print(f\"Dataset después de eliminar ubicaciones incorrectas: {df.shape}\")"
   ],
   "id": "f62230331a18055c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.7 - Conversión y optimización de tipos de datos**\n",
    "Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "- **Enteros:** Se usan los tipos más eficientes (`Int8`, `Int16`, `Int32`).\n",
    "- **Flotantes:** Se mantiene `float64` solo en coordenadas (`lat`, `lng`).\n",
    "- **Booleanos:** Se convierten variables binarias a `bool`.\n",
    "- **Categóricas:** `type`, `district` y `subdistrict` se transforman a `category`."
   ],
   "id": "faa46f73f2e9d826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "# - Columnas enteras: Se asigna el tipo más eficiente según el rango de valores observados.\n",
    "# - Columnas flotantes: Se mantienen en float64 solo las coordenadas, ya que requieren precisión.\n",
    "# - Columnas booleanas: Se convierten a bool para representar valores binarios.\n",
    "# - Columnas categóricas: Se transforman a 'category' para optimizar análisis y almacenamiento.\n",
    "\n",
    "# Columnas enteras\n",
    "df['price'] = df['price'].astype('Int32')\n",
    "df['floor_built'] = df['floor_built'].astype('Int16')\n",
    "df['floor'] = df['floor'].astype('Int8')\n",
    "df['bedrooms'] = df['bedrooms'].astype('Int8')\n",
    "df['bathrooms'] = df['bathrooms'].astype('Int8')\n",
    "df['deposit'] = df['deposit'].astype('Int8')\n",
    "\n",
    "# Columnas flotantes\n",
    "df['floor_area'] = df['floor_area'].astype('float32')\n",
    "df['lat'] = df['lat'].astype('float64') # Mantengo la precisión alta para las coordenadas\n",
    "df['lng'] = df['lng'].astype('float64')\n",
    "\n",
    "# Columnas booleanas\n",
    "bool_columns = ['second_hand', 'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                'storeroom', 'swimming_pool', 'garden_area', 'private_owner']\n",
    "for col in bool_columns:\n",
    "    df[col] = df[col].astype('bool')\n",
    "\n",
    "# Columnas categóricas\n",
    "categorical_columns = ['type', 'district', 'subdistrict']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Reviso cambios\n",
    "print(\"Tipos de datos después de la conversión:\")\n",
    "print(df.dtypes)"
   ],
   "id": "26ddbb6595f8498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3 - Análisis Exploratorio de Datos (EDA)**\n",
    "\n",
    "En esta sección se realiza un **análisis detallado de las características del dataset** mediante estadísticas descriptivas y visualizaciones gráficas. Se busca comprender la distribución de los datos, detectar valores atípicos y analizar relaciones entre variables clave.\n"
   ],
   "id": "bb36a9c426ea775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.1 - Análisis univariado de variables**\n",
    "Se estudia la distribución de cada variable individualmente para entender su comportamiento:\n",
    "- **Histogramas** y **boxplots** (`sns.histplot`, `sns.boxplot`) para analizar la forma y dispersión de las distribuciones.\n",
    "- **Identificación de valores atípicos (outliers)** de forma visual."
   ],
   "id": "c11c055458d338d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Lista de variables numéricas a analizar\n",
    "# num_vars = ['price', 'deposit', 'floor_built', 'floor_area', 'floor', 'bedrooms', 'bathrooms']\n",
    "#\n",
    "# # Crear figuras para histogramas y boxplots\n",
    "# fig, axes = plt.subplots(nrows=len(num_vars), ncols=2, figsize=(12, len(num_vars) * 4))\n",
    "#\n",
    "# # Generar gráficos para cada variable numérica\n",
    "# for i, var in enumerate(num_vars):\n",
    "#     # Histograma con KDE\n",
    "#     sns.histplot(df[var], bins=30, kde=True, ax=axes[i, 0], color='steelblue')\n",
    "#     axes[i, 0].set_title(f'Histograma de {var}')\n",
    "#\n",
    "#     # Boxplot\n",
    "#     sns.boxplot(x=df[var], ax=axes[i, 1], color='coral')\n",
    "#     axes[i, 1].set_title(f'Boxplot de {var}')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "6dde64e54e49df9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.2 - Análisis bivariado y relaciones entre variables**\n",
    "Se exploran correlaciones entre variables para detectar patrones y posibles predictores:\n",
    "- **Diagramas de dispersión** (`sns.scatterplot`) entre `price` y variables relevantes (`floor_area`, `bathrooms`, `bedrooms`).\n",
    "- **Matriz de correlación** (`sns.heatmap`) para visualizar relaciones numéricas entre las variables."
   ],
   "id": "1fa1babe1461367b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Matriz de correlación\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# corr_matrix = df.corr(numeric_only=True)\n",
    "#\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, annot_kws={\"size\": 8})\n",
    "# plt.xticks(rotation=45, ha=\"right\")\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.title(\"Matriz de Correlación entre Variables Numéricas\", fontsize=14)\n",
    "# plt.show()"
   ],
   "id": "ed660127f89a842b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Variables numéricas más correlacionadas con price\n",
    "# key_vars = ['floor_area', 'bedrooms', 'bathrooms', 'floor_built']\n",
    "#\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "#\n",
    "# for i, var in enumerate(key_vars):\n",
    "#     row, col = divmod(i, 2)\n",
    "#     sns.scatterplot(data=df, x=var, y='price', alpha=0.5, ax=axes[row, col], color='royalblue')\n",
    "#     sns.regplot(data=df, x=var, y='price', scatter=False, ax=axes[row, col], color='red', line_kws={\"alpha\":0.7})\n",
    "#     axes[row, col].set_title(f'Relación entre Price y {var}')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "376e057680ba9fa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Pairplot con las principales variables\n",
    "# sns.pairplot(df, vars=['price', 'floor_area', 'bedrooms', 'bathrooms', 'floor_built'], diag_kind=\"kde\", corner=True)\n",
    "# plt.show()"
   ],
   "id": "dc6b8d3b4e063cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3.3 - Análisis multivariante y relaciones complejas**\n",
   "id": "d694dd33b44603c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.4 - Mapa interactivo con coordenadas de viviendas**\n",
    "Se genera una visualización geográfica de la distribución de viviendas en Madrid:\n",
    "- Uso de **Folium** para trazar ubicaciones en un mapa.\n",
    "- Aplicación de **MarkerCluster** para agrupar zonas con alta densidad de viviendas."
   ],
   "id": "53efb3ca121c706b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO Crear la nueva columna con la distancia al centro de Madrid. Eliminar valores fuera de Madrid\n",
    "\n",
    "# Coordenadas del centro de Madrid\n",
    "madrid_center = [40.4168, -3.7038]\n",
    "\n",
    "# Crear un mapa base con zoom adecuado\n",
    "m_cluster = folium.Map(location=madrid_center, zoom_start=12, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Crear un objeto de clustering para agrupar los puntos\n",
    "marker_cluster = MarkerCluster().add_to(m_cluster)\n",
    "\n",
    "# Agregar marcadores agrupados con información relevante\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row[\"lat\"]) and pd.notna(row[\"lng\"]):  # Asegurar que las coordenadas no sean nulas\n",
    "        folium.Marker(\n",
    "            location=[row[\"lat\"], row[\"lng\"]],\n",
    "            popup=(\n",
    "                f\"<b>Precio:</b> {row['price']}€<br>\"\n",
    "                f\"<b>Superficie:</b> {row['floor_area']} m²<br>\"\n",
    "                f\"<b>Habitaciones:</b> {row['bedrooms']} | Baños: {row['bathrooms']}<br>\"\n",
    "                f\"<b>Distrito:</b> {row['district'].capitalize() if pd.notna(row['district']) else 'Desconocido'}<br>\"\n",
    "                f\"<b>Subdistrito:</b> {row['subdistrict'].capitalize() if pd.notna(row['subdistrict']) else 'Desconocido'}\"\n",
    "            ),\n",
    "            icon=folium.Icon(color=\"blue\", icon=\"home\"),\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "# Mostrar el mapa interactivo con clustering\n",
    "m_cluster"
   ],
   "id": "9c0b2556a3d8e18c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 - Preparación de los datos para el modelado",
   "id": "ad1e09b991c2080a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 - Codificación de variables categóricas",
   "id": "7b5b4b6e233e2415"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 - Escalado y normalización de variables numéricas",
   "id": "ca834d05ecd3b9f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 - Imputación de valores nulos",
   "id": "82ebe21dd8192af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.1 - Análisis de valores nulos por columna",
   "id": "4740611dc818c82d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar columnas con valores nulos mayores a 0\n",
    "null_values = df.isnull().sum()\n",
    "null_values = null_values[null_values > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Número de valores nulos por columna:\")\n",
    "print(null_values)"
   ],
   "id": "5979d6f0653c7f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar columnas con porcentaje de nulos mayor a 0\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "null_percentage = null_percentage[null_percentage > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage)"
   ],
   "id": "3d54963fa6337868",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.2 - deposit",
   "id": "37f5f7197e68cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### 2.7.2 - deposit\n",
    "# Cálculo de estadísticas descriptivas de 'deposit'\n",
    "# Se analiza tendencia central y dispersión para decidir la mejor estrategia de imputación.\n",
    "print(\"Moda:\", df['deposit'].mode()[0])\n",
    "print(\"Mediana:\", df['deposit'].median())\n",
    "print(\"Media:\", df['deposit'].mean())\n",
    "print(\"Varianza:\", df['deposit'].var())\n",
    "print(\"Desviación estándar:\", df['deposit'].std())\n",
    "print(\"Rango:\", df['deposit'].max() - df['deposit'].min())"
   ],
   "id": "6111f49c4493fd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Frecuencia absoluta\n",
    "print(df['deposit'].value_counts())\n",
    "\n",
    "# Frecuencia en porcentaje\n",
    "print(df['deposit'].value_counts(normalize=True) * 100)"
   ],
   "id": "b5ca97647f8df237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ver correlaciones\n",
    "print(df.select_dtypes(include=['number']).corr()['deposit'].sort_values(ascending=False))"
   ],
   "id": "6757df0636ca8fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Análisis de la dispersión de 'deposit' mediante percentiles e IQR\n",
    "# Esto ayuda a entender su distribución y detectar posibles valores atípicos.\n",
    "print(\"Percentil 25:\", df['deposit'].quantile(0.25))\n",
    "print(\"Percentil 50 (Mediana):\", df['deposit'].quantile(0.50))\n",
    "print(\"Percentil 75:\", df['deposit'].quantile(0.75))\n",
    "print(\"IQR:\", df['deposit'].quantile(0.75) - df['deposit'].quantile(0.25))"
   ],
   "id": "cdeb21bad2bd8b15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imputación de valores nulos en 'deposit'\n",
    "#\n",
    "# La mayoría de los valores en 'deposit' son 1 (59% de los casos), seguido de 2 (35%).\n",
    "# Dado que la media (1.47) está sesgada por unos pocos valores más altos y la mediana también es 1,\n",
    "# se decide imputar los valores nulos con la moda (1), ya que refleja mejor la tendencia real de los datos.\n",
    "df['deposit'] = df['deposit'].fillna(1)"
   ],
   "id": "f9e5990a9e1c01ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.3 - floor_build",
   "id": "eb6cfbbbd3c43342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dado que los valores nulos representan solo el 0.1% de la columna, se imputan con la mediana para mantener la distribución sin sesgos y evitar la influencia de valores atípicos.\n",
    "median_floor_built = df['floor_built'].median()\n",
    "df['floor_built'] = df['floor_built'].fillna(median_floor_built)"
   ],
   "id": "2cf65df49629c55f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.4 - floor_area",
   "id": "1d1d6a6d651ad240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# correlación con price (0.72), bedrooms (0.51), bathrooms (0.69) y floor_built (0.70)\n",
    "\n",
    "# Crear figura con 2 filas y 3 columnas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # Se ajustó el tamaño para mejor proporción\n",
    "\n",
    "# Gráfico de distribución de floor_area\n",
    "sns.histplot(df['floor_area'].dropna(), bins=30, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Distribución de Floor Area\")\n",
    "\n",
    "# Scatterplots con línea de tendencia\n",
    "sns.regplot(x=df['price'], y=df['floor_area'], ax=axes[0, 1], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[0, 1].set_title(\"Floor Area vs Price\")\n",
    "\n",
    "sns.regplot(x=df['bedrooms'], y=df['floor_area'], ax=axes[0, 2], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[0, 2].set_title(\"Floor Area vs Bedrooms\")\n",
    "\n",
    "sns.regplot(x=df['bathrooms'], y=df['floor_area'], ax=axes[1, 0], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[1, 0].set_title(\"Floor Area vs Bathrooms\")\n",
    "\n",
    "sns.regplot(x=df['floor_built'], y=df['floor_area'], ax=axes[1, 1], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[1, 1].set_title(\"Floor Area vs Floor Built\")\n",
    "\n",
    "# Matriz de correlación\n",
    "corr_matrix = df[['floor_area', 'price', 'bedrooms', 'bathrooms', 'floor_built']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, ax=axes[1, 2])\n",
    "axes[1, 2].set_title(\"Matriz de Correlación\")\n",
    "\n",
    "# Ajustar espaciado entre gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "49c7d6e9d45986c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se elige la Regresión Lineal Múltiple para imputar floor_area porque presenta una fuerte correlación con price (0.73),\n",
    "# bedrooms (0.74), bathrooms (0.79) y floor_built (0.92).\n",
    "# Las gráficas muestran una relación lineal clara entre floor_area y estas variables, lo que indica que pueden predecirlo de manera precisa.\n",
    "# Además, la distribución de floor_area está sesgada a la derecha, por lo que métodos como la mediana podrían subestimar los valores más altos.\n",
    "\n",
    "# 1Selección de las Variables Predictoras y Variable Dependiente\n",
    "predictors = ['price', 'bedrooms', 'bathrooms', 'floor_built']\n",
    "target = 'floor_area'\n",
    "\n",
    "# División del Dataset\n",
    "df_train = df.dropna(subset=[target])  # Filas sin nulos en floor_area (para entrenar)\n",
    "df_missing = df[df[target].isna()]  # Filas con nulos en floor_area (para predecir)\n",
    "\n",
    "# Preparación de los Datos para el Modelo\n",
    "X_train = df_train[predictors]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# Entrenamiento del Modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación del Modelo\n",
    "y_pred_train = model.predict(X_train)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "mape = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100  # Error absoluto porcentual medio\n",
    "\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")  # Muestra el error en porcentaje\n",
    "\n",
    "# 6️⃣ Predicción e Imputación de Valores Nulos\n",
    "if not df_missing.empty:\n",
    "    X_missing = df_missing[predictors]\n",
    "    df.loc[df[target].isna(), target] = model.predict(X_missing).astype('float32')"
   ],
   "id": "efb9649647157a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  Evaluación del Modelo de Imputación para `floor_area`\n",
    "# - R² = 0.872 → El modelo explica el 87.2% de la variabilidad en `floor_area`, lo que indica un buen ajuste.\n",
    "# - MAE = 9.47 → En promedio, el error absoluto en la imputación es de 9.47 m², un error moderado en la escala de la variable.\n",
    "# - RMSE = 26.14 → El error cuadrático medio sugiere que existen algunos valores más alejados de la realidad (posiblemente por outliers).\n",
    "# - MAPE = 10.21% → En promedio, los valores imputados tienen un error del 10.21% respecto a los valores reales,\n",
    "#   lo que indica una imputación razonablemente precisa, aunque con margen de mejora en valores extremos.\n",
    "#\n",
    "#   Conclusión: La regresión lineal múltiple ofrece una buena precisión en la imputación de `floor_area`, con un R² alto y errores aceptables.\n",
    "#   Sin embargo, los valores más altos de `floor_area` podrían estar generando desviaciones en la predicción.\n",
    "#   Se recomienda revisar outliers y considerar técnicas más avanzadas (como regresión robusta o árboles de decisión) para mejorar la estimación.\n"
   ],
   "id": "9c82ce87fdf43d8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.5 - floor\n",
   "id": "1e9ec18150469f78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 - Modelado",
   "id": "2887ed7741727105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8fc31a1df3a43384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 - Evaluación y conclusiones",
   "id": "7e5d1b29cf1605ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60a067cba282924a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
