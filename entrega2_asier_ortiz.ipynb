{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Mejoras para este ejercicio:\n",
    "# - Asegurar que los datos sean consistentes: si hay valores err√≥neos o nulos, establecer un random_state para garantizar reproducibilidad\n",
    "# - Implementar pipelines para estructurar mejor el flujo de preprocesamiento y modelado, evitando la aplicaci√≥n manual de cada paso\n",
    "# - Optimizar el modelo ajustando hiperpar√°metros con t√©cnicas como GridSearchCV o RandomizedSearchCV\n",
    "# - Utilizar Regex para validaciones: c√≥digos postales, tel√©fonos, emails, etc.\n",
    "# - Crear variables derivadas como precio por metro cuadrado (precio_m2 = precio / superficie)\n",
    "# - Geolocalizaci√≥n: obtener coordenadas con OpenStreetMap a partir de direcciones o c√≥digos postales y utilizarlas para an√°lisis espaciales\n",
    "# - Visualizar las viviendas en un mapa interactivo con Folium o Plotly Express para identificar patrones geogr√°ficos en los precios\n",
    "# - Clusterizaci√≥n de zonas con K-Means o DBSCAN para detectar patrones de precios por ubicaci√≥n y segmentar mejor los inmuebles\n",
    "# - Evitar data leakage: Dividir los datos en train/test antes de hacer encoding, eliminar outliers o escalar,\n",
    "#   asegurando que las transformaciones se ajusten s√≥lo con el conjunto de entrenamiento y luego se apliquen en test\n",
    "# - Subir el proyecto final a Kaggle"
   ],
   "id": "be40bb5aac5b5e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Funciones Helper**\n",
    "Aqu√≠ se encuentran las funciones auxiliares utilizadas en este notebook. Estas funciones permiten realizar tareas espec√≠ficas de manera m√°s ordenada y modular."
   ],
   "id": "dedb5826bbe4b858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_floor_column(df):\n",
    "    \"\"\"\n",
    "    Limpia la columna 'floor' asegurando consistencia en los valores.\n",
    "    - Convierte valores ordinales como '1st', '2nd', '3rd' en n√∫meros.\n",
    "    - Sustituye el valor 'ground' por 0.\n",
    "    - Convierte en NaN otros valores como 'floor'.\n",
    "    - Maneja valores extremadamente altos o negativos.\n",
    "    - Convierte a tipo Int16 para optimizar memoria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mostrar valores √∫nicos antes del saneamiento\n",
    "    print(\"\\nValores √∫nicos en 'floor' antes del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    # Diccionario de conversi√≥n de valores ordinales (1st, 2nd, etc.)\n",
    "    ordinal_mapping = {f\"{i}th\": i for i in range(1, 101)}\n",
    "    ordinal_mapping.update({\"ground\": 0, \"1st\": 1, \"2nd\": 2, \"3rd\": 3})\n",
    "\n",
    "    # Convertir valores de la columna\n",
    "    cleaned_floors = []\n",
    "    for value in df[\"floor\"]:\n",
    "        if pd.isna(value):\n",
    "            cleaned_floors.append(pd.NA)  # Mantener valores nulos\n",
    "            continue\n",
    "\n",
    "        value = str(value).lower().strip()\n",
    "\n",
    "        # Intentar convertir directamente si es un n√∫mero\n",
    "        try:\n",
    "            num_value = int(value.replace(\",\", \"\"))  # Manejo de valores con comas como \"1,200\"\n",
    "            if 0 <= num_value <= 100:  # Limitar a un rango razonable de pisos\n",
    "                cleaned_floors.append(num_value)\n",
    "            else:\n",
    "                cleaned_floors.append(pd.NA)  # Si el n√∫mero es absurdo, lo dejamos como NA\n",
    "        except ValueError:\n",
    "            cleaned_floors.append(ordinal_mapping.get(value, pd.NA))  # Mapear valores ordinales o NA si no se reconoce\n",
    "\n",
    "    # Asignar la columna limpia al DataFrame\n",
    "    df[\"floor\"] = cleaned_floors\n",
    "\n",
    "    # Convertir a Int16 para optimizar memoria, manteniendo NaN con pd.NA\n",
    "    df[\"floor\"] = df[\"floor\"].astype(\"Int16\") # TODO Hacer esto en la conversi√≥n\n",
    "\n",
    "    # Mostrar valores √∫nicos despu√©s del saneamiento\n",
    "    print(\"\\nValores √∫nicos en 'floor' despu√©s del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    return df"
   ],
   "id": "d3321250ab1bbe27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 - Carga de datos y revisi√≥n de la estructura del dataset\n",
    "\n",
    "En este apartado se realiza una **primera exploraci√≥n del dataset** para comprender su estructura, tipo de variables y posibles relaciones entre ellas. Este paso es fundamental para la correcta preparaci√≥n de los datos antes del modelado.\n",
    "\n",
    "### **1.1 - Importaci√≥n de librer√≠as y configuraci√≥n**\n",
    "Se importan las librer√≠as esenciales para el an√°lisis de datos, la visualizaci√≥n y el modelado, incluyendo **Pandas, NumPy, Matplotlib, Seaborn, SciPy y Scikit-Learn**. Adem√°s, se configuran algunos par√°metros globales de visualizaci√≥n para mejorar la legibilidad de los gr√°ficos.\n",
    "\n",
    "### **1.2 - Carga del dataset**\n",
    "Se carga el conjunto de datos en un **DataFrame de Pandas** desde un archivo CSV. Se incluye una referencia a la fuente del dataset.\n",
    "\n",
    "### **1.3 - Examinar la estructura del dataset**\n",
    "En esta fase se inspecciona la estructura general del dataset para entender su contenido y formato:\n",
    "\n",
    "- **Visualizaci√≥n de las primeras y √∫ltimas filas** para detectar posibles errores en la carga de los datos.\n",
    "- **Informaci√≥n general del dataset** (`df.info()`), que muestra el n√∫mero de registros, tipos de datos y valores nulos.\n",
    "- **N√∫mero total de filas y columnas** (`df.shape`).\n",
    "- **Identificaci√≥n de columnas num√©ricas y categ√≥ricas**, que ayudar√° en el preprocesamiento.\n",
    "- **Recuento de valores √∫nicos en variables categ√≥ricas**, √∫til para evaluar su diversidad.\n",
    "- **Visualizaci√≥n de los valores √∫nicos en algunas columnas categ√≥ricas clave**, como `type`, `floor`, `orientation`, `district` y `subdistrict`, con el objetivo de analizar su distribuci√≥n y detectar posibles inconsistencias.\n",
    "- **Revisi√≥n del n√∫mero de valores nulos por columna** (`df.isnull().sum()`), lo que permitir√° identificar qu√© variables requieren imputaci√≥n o eliminaci√≥n en la etapa de preprocesamiento.\n",
    "- **Resumen estad√≠stico de las variables num√©ricas**, para analizar su distribuci√≥n, detectar valores at√≠picos y entender la escala de los datos.\n",
    "\n",
    "### **1.4 - Comprobaci√≥n de relaciones potenciales**\n",
    "Para evaluar la viabilidad del modelado, se analizan las correlaciones entre variables num√©ricas:\n",
    "\n",
    "- Se genera una **matriz de correlaci√≥n** (`df.corr()`) para medir la relaci√≥n entre variables.\n",
    "- Se identifican **las variables con mayor impacto en `price`** bas√°ndose en la correlaci√≥n:\n",
    "  - `floor_built` (0.70)\n",
    "  - `floor_area` (0.72)\n",
    "  - `bedrooms` (0.51)\n",
    "  - `bathrooms` (0.69)\n",
    "- Se confirma la pertinencia de `balcony` como una **variable categ√≥rica de clasificaci√≥n**, revisando la distribuci√≥n de sus valores."
   ],
   "id": "f207b07e6af43aa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 - Importaci√≥n de librer√≠as y otras configuraciones",
   "id": "68947ca93b1ee80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar las librer√≠as necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Configuraciones\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)"
   ],
   "id": "cae123131f2ff1e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 - Carga del dataset",
   "id": "642fbdc3166ffe7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar el dataset local con Pandas\n",
    "df = pd.read_csv(\"scripts/madrid_rent_with_coordinates.csv\")\n",
    "\n",
    "# Ref. https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data"
   ],
   "id": "57f78ce23e88ed17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 - Examinar la estructura del dataset",
   "id": "dc595c93d116a8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las primeras filas para una vista inicial del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())"
   ],
   "id": "a652c94054a75007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las √∫ltimas filas para identificar posibles problemas en la carga de datos\n",
    "print(\"√öltimas filas del dataset:\")\n",
    "display(df.tail())"
   ],
   "id": "b29d30b31ad16871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Obtener informaci√≥n general sobre el dataset, incluyendo tipos de datos y valores nulos\n",
    "print(\"Informaci√≥n general del dataset:\")\n",
    "display(df.info())"
   ],
   "id": "cff1b45136ac14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el n√∫mero de filas y columnas en el dataset\n",
    "print(\"N√∫mero de filas y columnas en el dataset:\")\n",
    "print(df.shape)"
   ],
   "id": "d72fee52d985c328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar las columnas num√©ricas y categ√≥ricas\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ],
   "id": "6cc0a5878406bc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas num√©ricas identificadas\n",
    "print(\"Columnas num√©ricas:\")\n",
    "print(numerical_columns)"
   ],
   "id": "b8e474b4b602fdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas categ√≥ricas identificadas\n",
    "print(\"Columnas categ√≥ricas:\")\n",
    "print(categorical_columns)"
   ],
   "id": "3c8f87e02e5478fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar valores √∫nicos en variables categ√≥ricas\n",
    "print(\"Valores √∫nicos en variables categ√≥ricas:\")\n",
    "df[categorical_columns].nunique()"
   ],
   "id": "1e7904d2ca243a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores √∫nicos de algunas columnas categ√≥ricas para revisar su diversidad\n",
    "relevant_categorical_columns = ['type', 'floor', 'orientation', 'district', 'subdistrict']\n",
    "\n",
    "print(\"Valores √∫nicos en algunas variables categ√≥ricas relevantes:\")\n",
    "for col in relevant_categorical_columns:\n",
    "    print(f\"\\n{col}: {df[col].nunique()} valores √∫nicos\")\n",
    "    print(df[col].unique()[:10])"
   ],
   "id": "d0f9ab1f50b72cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el n√∫mero de valores nulos por columna\n",
    "print(df.isnull().sum())"
   ],
   "id": "aeed5067783abd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Describir estad√≠sticamente las variables num√©ricas para analizar su distribuci√≥n y posibles valores at√≠picos\n",
    "print(\"Resumen estad√≠stico de las variables num√©ricas:\")\n",
    "display(df.describe())"
   ],
   "id": "5f6b3ad50b6bc66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 - Comprobar relaciones potenciales",
   "id": "3c939f1df9844c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se genera la matriz de correlaci√≥n para analizar la relaci√≥n entre las variables num√©ricas\n",
    "print(\"Matriz de correlaci√≥n entre variables num√©ricas:\")\n",
    "correlation_matrix = df[numerical_columns].corr()\n",
    "display(correlation_matrix)"
   ],
   "id": "b88cd433b80c151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar relaciones relevantes para modelado\n",
    "\n",
    "# Bas√°ndonos en la matriz de correlaci√≥n, seleccionamos las variables con mayor impacto en 'price'\n",
    "# Seg√∫n el an√°lisis previo, las variables con correlaci√≥n m√°s fuerte con 'price' son:\n",
    "# - floor_built (0.70)\n",
    "# - floor_area (0.72)\n",
    "# - bedrooms (0.51)\n",
    "# - bathrooms (0.69)\n",
    "# Otras variables tienen correlaciones insignificantes (<0.01) y no se consideran para modelado\n",
    "\n",
    "key_relationships = ['price', 'floor_built', 'floor_area', 'bedrooms', 'bathrooms']\n",
    "print(\"Relaciones clave para la predicci√≥n de price:\")\n",
    "display(df[key_relationships].corr())"
   ],
   "id": "7f431580b67734b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se selecciona balcony como variable de clasificaci√≥n\n",
    "# Se analiza la frecuencia de los valores presentes en balcony para entender su distribuci√≥n\n",
    "print(\"Frecuencia de valores en la variable balcony:\")\n",
    "df['balcony'].value_counts()"
   ],
   "id": "99008d5a2872faa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 - Limpieza y validaci√≥n de los datos",
   "id": "5908d74e2a81ac48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 - Eliminaci√≥n de columnas irrelevantes",
   "id": "ef1f12f8be859629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se eliminan columnas que no aportan informaci√≥n √∫til para el an√°lisis de precios y balcones\n",
    "columns_to_drop = [\n",
    "    'web_id',               # Identificador √∫nico para cada anuncio, no aporta valor anal√≠tico\n",
    "    'url',                  # Enlace a la p√°gina del anuncio, irrelevante para el modelado\n",
    "    'title',                # Contiene informaci√≥n redundante, ya que 'type' y 'location' est√°n en otras columnas\n",
    "    'professional_name',    # Nombre de la agencia inmobiliaria o propietario, sin impacto en la predicci√≥n\n",
    "    'last_update',          # Indica la fecha de √∫ltima actualizaci√≥n del anuncio, pero sin un formato uniforme y sin valor predictivo\n",
    "    'location'              # Contiene el nombre de la calle y en algunos casos el n√∫mero, pero ya se tiene 'district' y 'subdistrict'\n",
    "]\n",
    "\n",
    "# Eliminamos las columnas del dataframe\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostramos el nuevo n√∫mero de columnas para verificar la eliminaci√≥n\n",
    "print(\"Columnas eliminadas:\", columns_to_drop)\n",
    "print(\"N√∫mero de columnas tras la eliminaci√≥n:\", df.shape[1])"
   ],
   "id": "f033ecda8ee99955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 - Manejo de duplicados",
   "id": "989a9fc4e765552d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar filas duplicadas considerando todas las columnas\n",
    "duplicates_total = df.duplicated().sum()\n",
    "print(f\"N√∫mero de filas completamente duplicadas: {duplicates_total}\")"
   ],
   "id": "7d37cac7a17b1ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Seleccionar columnas clave para identificar duplicados parciales excluyendo IDs o metadatos\n",
    "#\n",
    "# Para detectar duplicados, se consideran las columnas que describen las caracter√≠sticas estructurales de la vivienda,\n",
    "# evitando aquellas que son √∫nicas para cada anuncio o irrelevantes para el an√°lisis.\n",
    "#\n",
    "# Columnas **NO INCLUIDAS** en la detecci√≥n de duplicados y razones:\n",
    "# - `web_id` y `url`: Son identificadores √∫nicos de cada anuncio, por lo que no sirven para detectar duplicados.\n",
    "# - `title`: Contiene informaci√≥n redundante sobre el tipo de vivienda y ubicaci√≥n, pero en formato texto libre, lo que\n",
    "#   lo hace inconsistente para la comparaci√≥n.\n",
    "# - `professional_name`: El nombre de la agencia o propietario no influye en si un anuncio es duplicado o no.\n",
    "# - `last_update`: Fecha de actualizaci√≥n del anuncio con valores inconsistentes (por ejemplo, \"2 months\" vs. \"5 November\"),\n",
    "#   lo que impide su uso para identificar duplicados.\n",
    "# - `location`: Contiene el nombre de la calle y en algunos casos el n√∫mero, pero ya disponemos de `district` y `subdistrict`\n",
    "#   que son m√°s estructurados. Adem√°s, las direcciones pueden estar escritas de forma diferente en anuncios duplicados.\n",
    "#\n",
    "# Se opta por considerar caracter√≠sticas clave de la vivienda como `price`, `bedrooms`, `floor_area`, `year_built`, etc.,\n",
    "# ya que un mismo inmueble puede aparecer varias veces con ligeras variaciones en el t√≠tulo, la inmobiliaria o la fecha de publicaci√≥n.\n",
    "\n",
    "duplicate_columns = ['type', 'price', 'deposit', 'floor_built', 'floor_area',\n",
    "                     'year_built', 'bedrooms', 'bathrooms', 'second_hand',\n",
    "                     'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                     'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                     'storeroom', 'swimming_pool', 'garden_area', 'district', 'subdistrict',\n",
    "                     'postalcode', 'lat', 'lng']\n",
    "\n",
    "# Contar filas que tienen duplicaci√≥n en estas columnas\n",
    "duplicates_partial = df.duplicated(subset=duplicate_columns).sum()\n",
    "print(f\"N√∫mero de filas duplicadas considerando solo columnas clave: {duplicates_partial}\")"
   ],
   "id": "97db98a7af945d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar filas completamente duplicadas\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset despu√©s de eliminar duplicados completos: {df.shape}\")"
   ],
   "id": "a935b676f343b279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar duplicados en base a columnas clave, manteniendo la primera aparici√≥n\n",
    "df.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)\n",
    "print(f\"Dataset despu√©s de eliminar duplicados parciales: {df.shape}\")"
   ],
   "id": "243823915c0f68e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 - Eliminaci√≥n de columnas con alto porcentaje de nulos",
   "id": "581e0484cc64c14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "\n",
    "# Mostrar el porcentaje de nulos ordenado de mayor a menor\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage.sort_values(ascending=False))"
   ],
   "id": "92eef3b52f84fc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  floor_area (57%) ‚Üí Muy relevante para el precio. Mantener y evaluar imputaci√≥n aunque tenga un alto porcentaje de nulos\n",
    "#  deposit (41%) ‚Üí No es muy relevante para el precio. Pese a ello, mantener y evaluar imputaci√≥n aunque tenga un alto porcentaje de nulos\n",
    "\n",
    "# Definir las columnas a eliminar\n",
    "columns_to_drop = [\n",
    "    'year_built',      # Muchos nulos (68%). Aunque deber√≠a ser relevante para price (construcciones modernas podr√≠an ser m√°s caras) es dif√≠cil de imputar correctamente\n",
    "    'orientation',     # Muchos nulos (52%), dif√≠cil de imputar correctamente\n",
    "    'postalcode'       # Porcentaje de nulos moderadamente alto (25%). Redundante con district y subdistrict, no aporta informaci√≥n adicional\n",
    "]\n",
    "\n",
    "# Eliminar las columnas del DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostrar las columnas eliminadas y el nuevo n√∫mero de columnas\n",
    "print(\"Columnas eliminadas por alto porcentaje de valores nulos:\", columns_to_drop)\n",
    "print(\"N√∫mero de columnas despu√©s de la eliminaci√≥n:\", df.shape[1])\n"
   ],
   "id": "bab2a59024a96627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 - Saneamiento de valores en columnas categ√≥ricas",
   "id": "1315567ee7068831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneizaci√≥n de la columna 'type' para asegurar consistencia en los valores\n",
    "print(\"Valores √∫nicos en 'type' antes del saneamiento:\", df['type'].unique(), \"\\n\")\n",
    "\n",
    "# Elimino espacios en blanco y capitalizo los valores\n",
    "df['type'] = df['type'].str.strip().str.title()\n",
    "\n",
    "print(\"Valores √∫nicos en 'type' despu√©s del saneamiento:\", df['type'].unique())"
   ],
   "id": "4447283eec9bb833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Limpieza a la columna 'floor' para estandarizar valores, corregir inconsistencias y pasar los datos a num√©rico mediante la funci√≥n 'clear_floor_column'\n",
    "df = clean_floor_column(df)"
   ],
   "id": "dddc934de804cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneizaci√≥n b√°sica de 'district' y 'subdistrict'\n",
    "\n",
    "# Estas columnas solo se utilizar√°n en el an√°lisis exploratorio de datos (EDA),\n",
    "# por lo que aplicamos una limpieza m√≠nima para garantizar consistencia:\n",
    "# - Convertimos los valores a min√∫sculas para evitar diferencias por may√∫sculas/min√∫sculas.\n",
    "# - Eliminamos espacios en blanco adicionales para garantizar uniformidad.\n",
    "# - Reemplazamos valores 'nan' en formato string por valores NaN reales.\n",
    "# - No realizamos agrupaciones de nombres similares, ya que no afectar√°n al modelo.\n",
    "\n",
    "# Asegurar que sean de tipo 'object' antes de limpiar (evita problemas si son 'category')\n",
    "df['district'] = df['district'].astype(str).str.strip().str.lower()\n",
    "df['subdistrict'] = df['subdistrict'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Reemplazar valores 'nan' string por NaN reales de manera robusta\n",
    "df['district'] = df['district'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "df['subdistrict'] = df['subdistrict'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "\n",
    "# Mostramos los valores √∫nicos despu√©s del saneamiento\n",
    "print(\"Valores √∫nicos en 'district' despu√©s del saneamiento:\")\n",
    "display(df['district'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Valores √∫nicos en 'subdistrict' despu√©s del saneamiento:\")\n",
    "display(df['subdistrict'].unique())"
   ],
   "id": "6e742c95b92a7ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 - Saneamiento de valores en columnas num√©ricas",
   "id": "dcf7c9b60efd7137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores negativos donde no pueden existir\n",
    "not_neg_columns = ['price', 'deposit', 'floor_area', 'floor_built', 'floor', 'bedrooms', 'bathrooms']\n",
    "for col in not_neg_columns:\n",
    "    df.loc[df[col] < 0, col] = pd.NA"
   ],
   "id": "69b67e1fb540322b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores 0 en columnas donde no tiene sentido\n",
    "not_columns_zero = ['price', 'floor_area', 'floor_built']\n",
    "for col in not_columns_zero:\n",
    "    df.loc[df[col] == 0, col] = pd.NA"
   ],
   "id": "d3e48d4f9ccd58c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores negativos restantes\n",
    "for col in not_neg_columns:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    print(f\" {neg_count} valores negativos restantes en '{col}'.\")"
   ],
   "id": "d73d99ac208c5f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar cu√°ntos valores nulos quedaron en cada columna num√©rica saneada\n",
    "print(\"\\nValores nulos despu√©s del saneamiento:\")\n",
    "print(df[not_neg_columns].isna().sum())"
   ],
   "id": "9b8cfd976a1e7767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 - Conversi√≥n y optimizaci√≥n de tipos de datos",
   "id": "faa46f73f2e9d826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "# - Columnas enteras: Se asigna el tipo m√°s eficiente seg√∫n el rango de valores observados.\n",
    "# - Columnas flotantes: Se mantienen en float64 solo las coordenadas, ya que requieren precisi√≥n.\n",
    "# - Columnas booleanas: Se convierten a bool para representar valores binarios.\n",
    "# - Columnas categ√≥ricas: Se transforman a 'category' para optimizar an√°lisis y almacenamiento.\n",
    "\n",
    "# Columnas enteras\n",
    "df['price'] = df['price'].astype('Int32')\n",
    "df['floor_built'] = df['floor_built'].astype('Int16')\n",
    "df['floor'] = df['floor'].astype('Int8')\n",
    "df['bedrooms'] = df['bedrooms'].astype('Int8')\n",
    "df['bathrooms'] = df['bathrooms'].astype('Int8')\n",
    "df['deposit'] = df['deposit'].astype('Int8')\n",
    "df['floor_area'] = df['floor_area'].astype('Int16')\n",
    "\n",
    "# Columnas flotantes\n",
    "df['lat'] = df['lat'].astype('float64') # Mantengo la precisi√≥n para las coordenadas\n",
    "df['lng'] = df['lng'].astype('float64')\n",
    "\n",
    "# Columnas booleanas\n",
    "bool_columns = ['second_hand', 'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                'storeroom', 'swimming_pool', 'garden_area', 'private_owner']\n",
    "for col in bool_columns:\n",
    "    df[col] = df[col].astype('bool')\n",
    "\n",
    "# Columnas categ√≥ricas\n",
    "categorical_columns = ['type', 'district', 'subdistrict']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Reviso cambios\n",
    "print(\"Tipos de datos despu√©s de la conversi√≥n:\")\n",
    "print(df.dtypes)"
   ],
   "id": "26ddbb6595f8498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 - Imputaci√≥n de valores nulos",
   "id": "947bdab76f36b368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.7.1",
   "id": "46ec01bae0dbd0a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filtrar columnas con valores nulos mayores a 0\n",
    "null_values = df.isnull().sum()\n",
    "null_values = null_values[null_values > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"N√∫mero de valores nulos por columna:\")\n",
    "print(null_values)"
   ],
   "id": "de76c6b3c4c345b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filtrar columnas con porcentaje de nulos mayor a 0\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "null_percentage = null_percentage[null_percentage > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage)"
   ],
   "id": "a63424d193e5c6f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.7.2 - Deposit",
   "id": "ed74a2333a35765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Moda:\", df['deposit'].mode()[0])\n",
    "print(\"Mediana:\", df['deposit'].median())\n",
    "print(\"Media:\", df['deposit'].mean())\n",
    "print(\"Varianza:\", df['deposit'].var())\n",
    "print(\"Desviaci√≥n est√°ndar:\", df['deposit'].std())\n",
    "print(\"Rango:\", df['deposit'].max() - df['deposit'].min())"
   ],
   "id": "48ab9fc5492ad6b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Frecuencia absoluta\n",
    "print(df['deposit'].value_counts())\n",
    "# Frecuencia en porcentaje\n",
    "print(df['deposit'].value_counts(normalize=True) * 100)"
   ],
   "id": "e52379fcd5558e11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ver correlaciones\n",
    "print(df.select_dtypes(include=['number']).corr()['deposit'].sort_values(ascending=False))"
   ],
   "id": "26e8d13fb64a546f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Percentil 25:\", df['deposit'].quantile(0.25))\n",
    "print(\"Percentil 50 (Mediana):\", df['deposit'].quantile(0.50))\n",
    "print(\"Percentil 75:\", df['deposit'].quantile(0.75))\n",
    "print(\"IQR:\", df['deposit'].quantile(0.75) - df['deposit'].quantile(0.25))"
   ],
   "id": "af06469633c01925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO Para district y subdistrict tal vez podr√≠a sacarlo de otras colunmnas, que ya he eliminado... üòÇ",
   "id": "620b995333d06b03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
