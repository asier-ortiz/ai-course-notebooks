{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Mejoras para este ejercicio:\n",
    "# - Asegurar que los datos sean consistentes: si hay valores err√≥neos o nulos, establecer un random_state para garantizar reproducibilidad\n",
    "# - Implementar pipelines para estructurar mejor el flujo de preprocesamiento y modelado, evitando la aplicaci√≥n manual de cada paso\n",
    "# - Optimizar el modelo ajustando hiperpar√°metros con t√©cnicas como GridSearchCV o RandomizedSearchCV\n",
    "# - Utilizar Regex para validaciones: c√≥digos postales, tel√©fonos, emails, etc.\n",
    "# - Crear variables derivadas como precio por metro cuadrado (precio_m2 = precio / superficie)\n",
    "# - Geolocalizaci√≥n: obtener coordenadas con OpenStreetMap a partir de direcciones o c√≥digos postales y utilizarlas para an√°lisis espaciales\n",
    "# - Visualizar las viviendas en un mapa interactivo con Folium o Plotly Express para identificar patrones geogr√°ficos en los precios\n",
    "# - Clusterizaci√≥n de zonas con K-Means o DBSCAN para detectar patrones de precios por ubicaci√≥n y segmentar mejor los inmuebles\n",
    "# - Evitar data leakage: Dividir los datos en train/test antes de hacer encoding, eliminar outliers o escalar,\n",
    "#   asegurando que las transformaciones se ajusten s√≥lo con el conjunto de entrenamiento y luego se apliquen en test\n",
    "# - Subir el proyecto final a Kaggle"
   ],
   "id": "be40bb5aac5b5e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Funciones Helper**\n",
    "Aqu√≠ se encuentran las funciones auxiliares utilizadas en este notebook. Estas funciones permiten realizar tareas espec√≠ficas de manera m√°s ordenada y modular."
   ],
   "id": "dedb5826bbe4b858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_floor_column(df):\n",
    "    \"\"\"\n",
    "    Limpia la columna 'floor' asegurando consistencia en los valores.\n",
    "    - Convierte valores ordinales como '1st', '2nd', '3rd' en n√∫meros.\n",
    "    - Sustituye el valor 'ground' por 0.\n",
    "    - Convierte en NaN otros valores como 'floor'.\n",
    "    - Maneja valores extremadamente altos o negativos.\n",
    "    - Convierte a tipo Int16 para optimizar memoria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mostrar valores √∫nicos antes del saneamiento\n",
    "    print(\"\\nValores √∫nicos en 'floor' antes del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    # Diccionario de conversi√≥n de valores ordinales (1st, 2nd, etc.)\n",
    "    ordinal_mapping = {f\"{i}th\": i for i in range(1, 101)}\n",
    "    ordinal_mapping.update({\"ground\": 0, \"1st\": 1, \"2nd\": 2, \"3rd\": 3})\n",
    "\n",
    "    # Convertir valores de la columna\n",
    "    cleaned_floors = []\n",
    "    for value in df[\"floor\"]:\n",
    "        if pd.isna(value):\n",
    "            cleaned_floors.append(pd.NA)  # Mantener valores nulos\n",
    "            continue\n",
    "\n",
    "        value = str(value).lower().strip()\n",
    "\n",
    "        # Intentar convertir directamente si es un n√∫mero\n",
    "        try:\n",
    "            num_value = int(value.replace(\",\", \"\"))  # Manejo de valores con comas como \"1,200\"\n",
    "            if 0 <= num_value <= 100:  # Limitar a un rango razonable de pisos\n",
    "                cleaned_floors.append(num_value)\n",
    "            else:\n",
    "                cleaned_floors.append(pd.NA)  # Si el n√∫mero es absurdo, lo dejamos como NA\n",
    "        except ValueError:\n",
    "            cleaned_floors.append(ordinal_mapping.get(value, pd.NA))  # Mapear valores ordinales o NA si no se reconoce\n",
    "\n",
    "    # Asignar la columna limpia al DataFrame\n",
    "    df[\"floor\"] = cleaned_floors\n",
    "\n",
    "    # Convertir a Int16 para optimizar memoria, manteniendo NaN con pd.NA\n",
    "    df[\"floor\"] = df[\"floor\"].astype(\"Int16\") # TODO Hacer esto en la conversi√≥n\n",
    "\n",
    "    # Mostrar valores √∫nicos despu√©s del saneamiento\n",
    "    print(\"\\nValores √∫nicos en 'floor' despu√©s del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    return df"
   ],
   "id": "d3321250ab1bbe27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 - Carga de datos y revisi√≥n de la estructura del dataset\n",
    "\n",
    "En este apartado se realiza una **primera exploraci√≥n del dataset** para comprender su estructura, tipo de variables y posibles relaciones entre ellas. Este paso es fundamental para la correcta preparaci√≥n de los datos antes del modelado."
   ],
   "id": "f207b07e6af43aa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.1 - Importaci√≥n de librer√≠as y configuraci√≥n**\n",
    "Se importan las librer√≠as esenciales para el an√°lisis de datos, la visualizaci√≥n y el modelado, incluyendo **Pandas, NumPy, Matplotlib, Seaborn, SciPy y Scikit-Learn**. Adem√°s, se configuran algunos par√°metros globales de visualizaci√≥n para mejorar la legibilidad de los gr√°ficos."
   ],
   "id": "68947ca93b1ee80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar las librer√≠as necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Configuraciones\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)"
   ],
   "id": "cae123131f2ff1e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.2 - Carga del dataset**\n",
    "Se carga el conjunto de datos en un **DataFrame de Pandas** desde un archivo CSV. Se incluye una referencia a la fuente del dataset."
   ],
   "id": "642fbdc3166ffe7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar el dataset local con Pandas\n",
    "df = pd.read_csv(\"scripts/madrid_rent_with_coordinates.csv\")\n",
    "\n",
    "# Ref. https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data"
   ],
   "id": "57f78ce23e88ed17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.3 - Inspecci√≥n de la estructura del dataset**\n",
    "En esta fase se inspecciona la estructura general del dataset para entender su contenido y formato:\n",
    "\n",
    "- **Visualizaci√≥n de las primeras y √∫ltimas filas** para detectar posibles errores en la carga de los datos.\n",
    "- **Informaci√≥n general del dataset** (`df.info()`), que muestra el n√∫mero de registros, tipos de datos y valores nulos.\n",
    "- **N√∫mero total de filas y columnas** (`df.shape`).\n",
    "- **Identificaci√≥n de columnas num√©ricas y categ√≥ricas**, que ayudar√° en el preprocesamiento.\n",
    "- **Recuento de valores √∫nicos en variables categ√≥ricas**, √∫til para evaluar su diversidad.\n",
    "- **Visualizaci√≥n de los valores √∫nicos en algunas columnas categ√≥ricas clave**, como `type`, `floor`, `orientation`, `district` y `subdistrict`, con el objetivo de analizar su distribuci√≥n y detectar posibles inconsistencias.\n",
    "- **Revisi√≥n del n√∫mero de valores nulos por columna** (`df.isnull().sum()`), lo que permitir√° identificar qu√© variables requieren imputaci√≥n o eliminaci√≥n en la etapa de preprocesamiento.\n",
    "- **Resumen estad√≠stico de las variables num√©ricas**, para analizar su distribuci√≥n, detectar valores at√≠picos y entender la escala de los datos."
   ],
   "id": "dc595c93d116a8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las primeras filas para una vista inicial del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())"
   ],
   "id": "a652c94054a75007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las √∫ltimas filas para identificar posibles problemas en la carga de datos\n",
    "print(\"√öltimas filas del dataset:\")\n",
    "display(df.tail())"
   ],
   "id": "b29d30b31ad16871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Obtener informaci√≥n general sobre el dataset, incluyendo tipos de datos y valores nulos\n",
    "print(\"Informaci√≥n general del dataset:\")\n",
    "display(df.info())"
   ],
   "id": "cff1b45136ac14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el n√∫mero de filas y columnas en el dataset\n",
    "print(\"N√∫mero de filas y columnas en el dataset:\")\n",
    "print(df.shape)"
   ],
   "id": "d72fee52d985c328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar las columnas num√©ricas y categ√≥ricas\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ],
   "id": "6cc0a5878406bc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas num√©ricas identificadas\n",
    "print(\"Columnas num√©ricas:\")\n",
    "print(numerical_columns)"
   ],
   "id": "b8e474b4b602fdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas categ√≥ricas identificadas\n",
    "print(\"Columnas categ√≥ricas:\")\n",
    "print(categorical_columns)"
   ],
   "id": "3c8f87e02e5478fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar valores √∫nicos en variables categ√≥ricas\n",
    "print(\"Valores √∫nicos en variables categ√≥ricas:\")\n",
    "df[categorical_columns].nunique()"
   ],
   "id": "1e7904d2ca243a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores √∫nicos de algunas columnas categ√≥ricas para revisar su diversidad\n",
    "relevant_categorical_columns = ['type', 'floor', 'orientation', 'district', 'subdistrict']\n",
    "\n",
    "print(\"Valores √∫nicos en algunas variables categ√≥ricas relevantes:\")\n",
    "for col in relevant_categorical_columns:\n",
    "    print(f\"\\n{col}: {df[col].nunique()} valores √∫nicos\")\n",
    "    print(df[col].unique()[:10])"
   ],
   "id": "d0f9ab1f50b72cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el n√∫mero de valores nulos por columna\n",
    "print(df.isnull().sum())"
   ],
   "id": "aeed5067783abd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Describir estad√≠sticamente las variables num√©ricas para analizar su distribuci√≥n y posibles valores at√≠picos\n",
    "print(\"Resumen estad√≠stico de las variables num√©ricas:\")\n",
    "display(df.describe())"
   ],
   "id": "5f6b3ad50b6bc66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1.4 - Comprobaci√≥n de relaciones potenciales**\n",
    "Para evaluar la viabilidad del modelado, se analizan las correlaciones entre variables num√©ricas:\n",
    "\n",
    "- Se genera una **matriz de correlaci√≥n** (`df.corr()`) para medir la relaci√≥n entre variables.\n",
    "- Se identifican **las variables con mayor impacto en `price`** bas√°ndose en la correlaci√≥n:\n",
    "  - `floor_built` (0.70)\n",
    "  - `floor_area` (0.72)\n",
    "  - `bedrooms` (0.51)\n",
    "  - `bathrooms` (0.69)\n",
    "- Se confirma la pertinencia de `balcony` como una **variable categ√≥rica de clasificaci√≥n**, revisando la distribuci√≥n de sus valores."
   ],
   "id": "3c939f1df9844c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se genera la matriz de correlaci√≥n para analizar la relaci√≥n entre las variables num√©ricas\n",
    "print(\"Matriz de correlaci√≥n entre variables num√©ricas:\")\n",
    "correlation_matrix = df[numerical_columns].corr()\n",
    "display(correlation_matrix)"
   ],
   "id": "b88cd433b80c151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar relaciones relevantes para modelado\n",
    "\n",
    "# Bas√°ndonos en la matriz de correlaci√≥n, seleccionamos las variables con mayor impacto en 'price'\n",
    "# Seg√∫n el an√°lisis previo, las variables con correlaci√≥n m√°s fuerte con 'price' son:\n",
    "# - floor_built (0.70)\n",
    "# - floor_area (0.72)\n",
    "# - bedrooms (0.51)\n",
    "# - bathrooms (0.69)\n",
    "# Otras variables tienen correlaciones insignificantes (<0.01) y no se consideran para modelado\n",
    "\n",
    "key_relationships = ['price', 'floor_built', 'floor_area', 'bedrooms', 'bathrooms']\n",
    "print(\"Relaciones clave para la predicci√≥n de price:\")\n",
    "display(df[key_relationships].corr())"
   ],
   "id": "7f431580b67734b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se selecciona balcony como variable de clasificaci√≥n\n",
    "# Se analiza la frecuencia de los valores presentes en balcony para entender su distribuci√≥n\n",
    "print(\"Frecuencia de valores en la variable balcony:\")\n",
    "df['balcony'].value_counts()"
   ],
   "id": "99008d5a2872faa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2 - Limpieza y validaci√≥n de los datos**\n",
    "\n",
    "Este apartado se centra en la **depuraci√≥n del dataset** para garantizar que los datos sean consistentes, relevantes y adecuados para el modelado. Se eliminan columnas innecesarias, se manejan valores duplicados y se corrigen inconsistencias en los valores."
   ],
   "id": "5908d74e2a81ac48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.1 - Eliminaci√≥n de columnas irrelevantes**\n",
    "Se eliminan columnas que no aportan informaci√≥n √∫til para el an√°lisis de precios y balcones. Estas incluyen identificadores √∫nicos (`web_id`), enlaces (`url`), informaci√≥n redundante (`title`), y metadatos sin valor predictivo (`professional_name`, `last_update`, `location`)."
   ],
   "id": "ef1f12f8be859629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se eliminan columnas que no aportan informaci√≥n √∫til para el an√°lisis de precios y balcones\n",
    "columns_to_drop = [\n",
    "    'web_id',               # Identificador √∫nico para cada anuncio, no aporta valor anal√≠tico\n",
    "    'url',                  # Enlace a la p√°gina del anuncio, irrelevante para el modelado\n",
    "    'title',                # Contiene informaci√≥n redundante, ya que 'type' y 'location' est√°n en otras columnas\n",
    "    'professional_name',    # Nombre de la agencia inmobiliaria o propietario, sin impacto en la predicci√≥n\n",
    "    'last_update',          # Indica la fecha de √∫ltima actualizaci√≥n del anuncio, pero sin un formato uniforme y sin valor predictivo\n",
    "    'location'              # Contiene el nombre de la calle y en algunos casos el n√∫mero, pero ya se tiene 'district' y 'subdistrict'\n",
    "]\n",
    "\n",
    "# Eliminamos las columnas del dataframe\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostramos el nuevo n√∫mero de columnas para verificar la eliminaci√≥n\n",
    "print(\"Columnas eliminadas:\", columns_to_drop)\n",
    "print(\"N√∫mero de columnas tras la eliminaci√≥n:\", df.shape[1])"
   ],
   "id": "f033ecda8ee99955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.2 - Manejo de duplicados**\n",
    "Se identifican y eliminan registros duplicados para evitar sesgos en el an√°lisis. Se revisan duplicados completos y duplicados parciales en base a caracter√≠sticas clave como `price`, `floor_area` y `bedrooms`."
   ],
   "id": "989a9fc4e765552d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar filas duplicadas considerando todas las columnas\n",
    "duplicates_total = df.duplicated().sum()\n",
    "print(f\"N√∫mero de filas completamente duplicadas: {duplicates_total}\")"
   ],
   "id": "7d37cac7a17b1ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Seleccionar columnas clave para identificar duplicados parciales excluyendo IDs o metadatos\n",
    "#\n",
    "# Para detectar duplicados, se consideran las columnas que describen las caracter√≠sticas estructurales de la vivienda,\n",
    "# evitando aquellas que son √∫nicas para cada anuncio o irrelevantes para el an√°lisis.\n",
    "#\n",
    "# Columnas **NO INCLUIDAS** en la detecci√≥n de duplicados y razones:\n",
    "# - `web_id` y `url`: Son identificadores √∫nicos de cada anuncio, por lo que no sirven para detectar duplicados.\n",
    "# - `title`: Contiene informaci√≥n redundante sobre el tipo de vivienda y ubicaci√≥n, pero en formato texto libre, lo que\n",
    "#   lo hace inconsistente para la comparaci√≥n.\n",
    "# - `professional_name`: El nombre de la agencia o propietario no influye en si un anuncio es duplicado o no.\n",
    "# - `last_update`: Fecha de actualizaci√≥n del anuncio con valores inconsistentes (por ejemplo, \"2 months\" vs. \"5 November\"),\n",
    "#   lo que impide su uso para identificar duplicados.\n",
    "# - `location`: Contiene el nombre de la calle y en algunos casos el n√∫mero, pero ya disponemos de `district` y `subdistrict`\n",
    "#   que son m√°s estructurados. Adem√°s, las direcciones pueden estar escritas de forma diferente en anuncios duplicados.\n",
    "#\n",
    "# Se opta por considerar caracter√≠sticas clave de la vivienda como `price`, `bedrooms`, `floor_area`, `year_built`, etc.,\n",
    "# ya que un mismo inmueble puede aparecer varias veces con ligeras variaciones en el t√≠tulo, la inmobiliaria o la fecha de publicaci√≥n.\n",
    "\n",
    "duplicate_columns = ['type', 'price', 'deposit', 'floor_built', 'floor_area',\n",
    "                     'year_built', 'bedrooms', 'bathrooms', 'second_hand',\n",
    "                     'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                     'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                     'storeroom', 'swimming_pool', 'garden_area', 'district', 'subdistrict',\n",
    "                     'postalcode', 'lat', 'lng']\n",
    "\n",
    "# Contar filas que tienen duplicaci√≥n en estas columnas\n",
    "duplicates_partial = df.duplicated(subset=duplicate_columns).sum()\n",
    "print(f\"N√∫mero de filas duplicadas considerando solo columnas clave: {duplicates_partial}\")"
   ],
   "id": "97db98a7af945d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar filas completamente duplicadas\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset despu√©s de eliminar duplicados completos: {df.shape}\")"
   ],
   "id": "a935b676f343b279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar duplicados en base a columnas clave, manteniendo la primera aparici√≥n\n",
    "df.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)\n",
    "print(f\"Dataset despu√©s de eliminar duplicados parciales: {df.shape}\")"
   ],
   "id": "243823915c0f68e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.3 - Eliminaci√≥n de columnas con alto porcentaje de valores nulos**\n",
    "Se eliminan variables con un gran n√∫mero de valores nulos que no pueden imputarse de forma fiable, como `year_built`, `orientation` y `postalcode`. Se justifica la retenci√≥n de otras variables con nulos si son clave para el an√°lisis."
   ],
   "id": "581e0484cc64c14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "\n",
    "# Mostrar el porcentaje de nulos ordenado de mayor a menor\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage.sort_values(ascending=False))"
   ],
   "id": "92eef3b52f84fc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  floor_area (57%) ‚Üí Muy relevante para el precio. Mantener y evaluar imputaci√≥n aunque tenga un alto porcentaje de nulos\n",
    "#  deposit (41%) ‚Üí No es muy relevante para el precio. Pese a ello, mantener y evaluar imputaci√≥n aunque tenga un alto porcentaje de nulos\n",
    "\n",
    "# Definir las columnas a eliminar\n",
    "columns_to_drop = [\n",
    "    'year_built',      # Muchos nulos (68%). Aunque deber√≠a ser relevante para price (construcciones modernas podr√≠an ser m√°s caras) es dif√≠cil de imputar correctamente\n",
    "    'orientation',     # Muchos nulos (52%), dif√≠cil de imputar correctamente\n",
    "    'postalcode'       # Porcentaje de nulos moderadamente alto (25%). Redundante con district y subdistrict, no aporta informaci√≥n adicional\n",
    "]\n",
    "\n",
    "# Eliminar las columnas del DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostrar las columnas eliminadas y el nuevo n√∫mero de columnas\n",
    "print(\"Columnas eliminadas por alto porcentaje de valores nulos:\", columns_to_drop)\n",
    "print(\"N√∫mero de columnas despu√©s de la eliminaci√≥n:\", df.shape[1])"
   ],
   "id": "bab2a59024a96627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.4 - Saneamiento de valores en columnas categ√≥ricas**\n",
    "Se homogeneizan valores en columnas categ√≥ricas como `type`, `district` y `subdistrict`, eliminando inconsistencias en may√∫sculas/min√∫sculas, espacios en blanco y valores incorrectos."
   ],
   "id": "1315567ee7068831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneizaci√≥n de la columna 'type' para asegurar consistencia en los valores\n",
    "print(\"Valores √∫nicos en 'type' antes del saneamiento:\", df['type'].unique(), \"\\n\")\n",
    "\n",
    "# Elimino espacios en blanco y capitalizo los valores\n",
    "df['type'] = df['type'].str.strip().str.title()\n",
    "\n",
    "print(\"Valores √∫nicos en 'type' despu√©s del saneamiento:\", df['type'].unique())"
   ],
   "id": "4447283eec9bb833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Limpieza a la columna 'floor' para estandarizar valores, corregir inconsistencias y pasar los datos a num√©rico mediante la funci√≥n 'clear_floor_column'\n",
    "df = clean_floor_column(df)"
   ],
   "id": "dddc934de804cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneizaci√≥n b√°sica de 'district' y 'subdistrict'\n",
    "\n",
    "# Estas columnas solo se utilizar√°n en el an√°lisis exploratorio de datos (EDA),\n",
    "# por lo que aplicamos una limpieza m√≠nima para garantizar consistencia:\n",
    "# - Convertimos los valores a min√∫sculas para evitar diferencias por may√∫sculas/min√∫sculas.\n",
    "# - Eliminamos espacios en blanco adicionales para garantizar uniformidad.\n",
    "# - Reemplazamos valores 'nan' en formato string por valores NaN reales.\n",
    "# - No realizamos agrupaciones de nombres similares, ya que no afectar√°n al modelo.\n",
    "\n",
    "# Asegurar que sean de tipo 'object' antes de limpiar (evita problemas si son 'category')\n",
    "df['district'] = df['district'].astype(str).str.strip().str.lower()\n",
    "df['subdistrict'] = df['subdistrict'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Reemplazar valores 'nan' string por NaN reales de manera robusta\n",
    "df['district'] = df['district'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "df['subdistrict'] = df['subdistrict'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "\n",
    "# Mostramos los valores √∫nicos despu√©s del saneamiento\n",
    "print(\"Valores √∫nicos en 'district' despu√©s del saneamiento:\")\n",
    "display(df['district'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Valores √∫nicos en 'subdistrict' despu√©s del saneamiento:\")\n",
    "display(df['subdistrict'].unique())"
   ],
   "id": "6e742c95b92a7ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.5 - Saneamiento de valores en columnas num√©ricas**\n",
    "Se sustituyen por `NaN` los valores negativos en columnas donde no pueden existir (e.g., `price`, `floor_area`, `bathrooms`). Adem√°s, se eliminan valores 0 en columnas donde no tienen sentido (`price`, `floor_area`, `floor_built`)."
   ],
   "id": "dcf7c9b60efd7137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores negativos donde no pueden existir\n",
    "not_neg_columns = ['price', 'deposit', 'floor_area', 'floor_built', 'floor', 'bedrooms', 'bathrooms']\n",
    "for col in not_neg_columns:\n",
    "    df.loc[df[col] < 0, col] = pd.NA"
   ],
   "id": "69b67e1fb540322b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores 0 en columnas donde no tiene sentido\n",
    "not_zero_columns = ['price', 'floor_area', 'floor_built']\n",
    "for col in not_zero_columns:\n",
    "    df.loc[df[col] == 0, col] = pd.NA"
   ],
   "id": "d3e48d4f9ccd58c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores negativos restantes\n",
    "for col in not_neg_columns:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    print(f\" {neg_count} valores negativos restantes en '{col}'.\")"
   ],
   "id": "d73d99ac208c5f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores 0 restantes en las columnas afectadas\n",
    "print(\"\\nValores 0 restantes en columnas afectadas:\")\n",
    "for col in not_zero_columns:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    print(f\" - {col}: {zero_count} valores con valor 0\")"
   ],
   "id": "b2794f8e35d43b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar cu√°ntos valores nulos quedaron en cada columna num√©rica saneada\n",
    "print(\"\\nValores nulos despu√©s del saneamiento:\")\n",
    "print(df[not_neg_columns].isna().sum())"
   ],
   "id": "9b8cfd976a1e7767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **2.6 - Conversi√≥n y optimizaci√≥n de tipos de datos**\n",
    "Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "- **Enteros:** Se usan los tipos m√°s eficientes (`Int8`, `Int16`, `Int32`).\n",
    "- **Flotantes:** Se mantiene `float64` solo en coordenadas (`lat`, `lng`).\n",
    "- **Booleanos:** Se convierten variables binarias a `bool`.\n",
    "- **Categ√≥ricas:** `type`, `district` y `subdistrict` se transforman a `category`."
   ],
   "id": "faa46f73f2e9d826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "# - Columnas enteras: Se asigna el tipo m√°s eficiente seg√∫n el rango de valores observados.\n",
    "# - Columnas flotantes: Se mantienen en float64 solo las coordenadas, ya que requieren precisi√≥n.\n",
    "# - Columnas booleanas: Se convierten a bool para representar valores binarios.\n",
    "# - Columnas categ√≥ricas: Se transforman a 'category' para optimizar an√°lisis y almacenamiento.\n",
    "\n",
    "# Columnas enteras\n",
    "df['price'] = df['price'].astype('Int32')\n",
    "df['floor_built'] = df['floor_built'].astype('Int16')\n",
    "df['floor'] = df['floor'].astype('Int8')\n",
    "df['bedrooms'] = df['bedrooms'].astype('Int8')\n",
    "df['bathrooms'] = df['bathrooms'].astype('Int8')\n",
    "df['deposit'] = df['deposit'].astype('Int8')\n",
    "\n",
    "# Columnas flotantes\n",
    "df['floor_area'] = df['floor_area'].astype('float32')\n",
    "df['lat'] = df['lat'].astype('float64') # Mantengo la precisi√≥n alta para las coordenadas\n",
    "df['lng'] = df['lng'].astype('float64')\n",
    "\n",
    "# Columnas booleanas\n",
    "bool_columns = ['second_hand', 'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                'storeroom', 'swimming_pool', 'garden_area', 'private_owner']\n",
    "for col in bool_columns:\n",
    "    df[col] = df[col].astype('bool')\n",
    "\n",
    "# Columnas categ√≥ricas\n",
    "categorical_columns = ['type', 'district', 'subdistrict']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Reviso cambios\n",
    "print(\"Tipos de datos despu√©s de la conversi√≥n:\")\n",
    "print(df.dtypes)"
   ],
   "id": "26ddbb6595f8498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 - An√°lisis Exploratorio de Datos (EDA)\n",
   "id": "bb36a9c426ea775"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d85cbbb0840ef207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 - Preparaci√≥n de los datos para el modelado",
   "id": "ad1e09b991c2080a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 - Codificaci√≥n de variables categ√≥ricas",
   "id": "7b5b4b6e233e2415"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 - Escalado y normalizaci√≥n de variables num√©ricas",
   "id": "ca834d05ecd3b9f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 - Imputaci√≥n de valores nulos",
   "id": "82ebe21dd8192af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.1 - An√°lisis de valores nulos por columna",
   "id": "4740611dc818c82d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar columnas con valores nulos mayores a 0\n",
    "null_values = df.isnull().sum()\n",
    "null_values = null_values[null_values > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"N√∫mero de valores nulos por columna:\")\n",
    "print(null_values)"
   ],
   "id": "5979d6f0653c7f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtrar columnas con porcentaje de nulos mayor a 0\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "null_percentage = null_percentage[null_percentage > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage)"
   ],
   "id": "3d54963fa6337868",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.2 - deposit",
   "id": "37f5f7197e68cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### 2.7.2 - deposit\n",
    "# C√°lculo de estad√≠sticas descriptivas de 'deposit'\n",
    "# Se analiza tendencia central y dispersi√≥n para decidir la mejor estrategia de imputaci√≥n.\n",
    "print(\"Moda:\", df['deposit'].mode()[0])\n",
    "print(\"Mediana:\", df['deposit'].median())\n",
    "print(\"Media:\", df['deposit'].mean())\n",
    "print(\"Varianza:\", df['deposit'].var())\n",
    "print(\"Desviaci√≥n est√°ndar:\", df['deposit'].std())\n",
    "print(\"Rango:\", df['deposit'].max() - df['deposit'].min())"
   ],
   "id": "6111f49c4493fd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Frecuencia absoluta\n",
    "print(df['deposit'].value_counts())\n",
    "\n",
    "# Frecuencia en porcentaje\n",
    "print(df['deposit'].value_counts(normalize=True) * 100)"
   ],
   "id": "b5ca97647f8df237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ver correlaciones\n",
    "print(df.select_dtypes(include=['number']).corr()['deposit'].sort_values(ascending=False))"
   ],
   "id": "6757df0636ca8fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# An√°lisis de la dispersi√≥n de 'deposit' mediante percentiles e IQR\n",
    "# Esto ayuda a entender su distribuci√≥n y detectar posibles valores at√≠picos.\n",
    "print(\"Percentil 25:\", df['deposit'].quantile(0.25))\n",
    "print(\"Percentil 50 (Mediana):\", df['deposit'].quantile(0.50))\n",
    "print(\"Percentil 75:\", df['deposit'].quantile(0.75))\n",
    "print(\"IQR:\", df['deposit'].quantile(0.75) - df['deposit'].quantile(0.25))"
   ],
   "id": "cdeb21bad2bd8b15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imputaci√≥n de valores nulos en 'deposit'\n",
    "#\n",
    "# La mayor√≠a de los valores en 'deposit' son 1 (59% de los casos), seguido de 2 (35%).\n",
    "# Dado que la media (1.47) est√° sesgada por unos pocos valores m√°s altos y la mediana tambi√©n es 1,\n",
    "# se decide imputar los valores nulos con la moda (1), ya que refleja mejor la tendencia real de los datos.\n",
    "df['deposit'] = df['deposit'].fillna(1)"
   ],
   "id": "f9e5990a9e1c01ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.3 - floor_build",
   "id": "eb6cfbbbd3c43342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dado que los valores nulos representan solo el 0.1% de la columna, se imputan con la mediana para mantener la distribuci√≥n sin sesgos y evitar la influencia de valores at√≠picos.\n",
    "median_floor_built = df['floor_built'].median()\n",
    "df['floor_built'] = df['floor_built'].fillna(median_floor_built)"
   ],
   "id": "2cf65df49629c55f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.4 - floor_area",
   "id": "1d1d6a6d651ad240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# correlaci√≥n con price (0.72), bedrooms (0.51), bathrooms (0.69) y floor_built (0.70)\n",
    "\n",
    "# Crear figura con 2 filas y 3 columnas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # Se ajust√≥ el tama√±o para mejor proporci√≥n\n",
    "\n",
    "# Gr√°fico de distribuci√≥n de floor_area\n",
    "sns.histplot(df['floor_area'].dropna(), bins=30, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Distribuci√≥n de Floor Area\")\n",
    "\n",
    "# Scatterplots con l√≠nea de tendencia\n",
    "sns.regplot(x=df['price'], y=df['floor_area'], ax=axes[0, 1], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[0, 1].set_title(\"Floor Area vs Price\")\n",
    "\n",
    "sns.regplot(x=df['bedrooms'], y=df['floor_area'], ax=axes[0, 2], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[0, 2].set_title(\"Floor Area vs Bedrooms\")\n",
    "\n",
    "sns.regplot(x=df['bathrooms'], y=df['floor_area'], ax=axes[1, 0], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[1, 0].set_title(\"Floor Area vs Bathrooms\")\n",
    "\n",
    "sns.regplot(x=df['floor_built'], y=df['floor_area'], ax=axes[1, 1], scatter_kws={'alpha':0.5}, line_kws={\"color\": \"red\"})\n",
    "axes[1, 1].set_title(\"Floor Area vs Floor Built\")\n",
    "\n",
    "# Matriz de correlaci√≥n\n",
    "corr_matrix = df[['floor_area', 'price', 'bedrooms', 'bathrooms', 'floor_built']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, ax=axes[1, 2])\n",
    "axes[1, 2].set_title(\"Matriz de Correlaci√≥n\")\n",
    "\n",
    "# Ajustar espaciado entre gr√°ficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "49c7d6e9d45986c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se elige la Regresi√≥n Lineal M√∫ltiple para imputar floor_area porque presenta una fuerte correlaci√≥n con price (0.73),\n",
    "# bedrooms (0.74), bathrooms (0.79) y floor_built (0.92).\n",
    "# Las gr√°ficas muestran una relaci√≥n lineal clara entre floor_area y estas variables, lo que indica que pueden predecirlo de manera precisa.\n",
    "# Adem√°s, la distribuci√≥n de floor_area est√° sesgada a la derecha, por lo que m√©todos como la mediana podr√≠an subestimar los valores m√°s altos.\n",
    "\n",
    "# 1Ô∏è‚É£ Selecci√≥n de las Variables Predictoras y Variable Dependiente\n",
    "predictors = ['price', 'bedrooms', 'bathrooms', 'floor_built']\n",
    "target = 'floor_area'\n",
    "\n",
    "# 2Ô∏è‚É£ Divisi√≥n del Dataset\n",
    "df_train = df.dropna(subset=[target])  # Filas sin nulos en floor_area (para entrenar)\n",
    "df_missing = df[df[target].isna()]  # Filas con nulos en floor_area (para predecir)\n",
    "\n",
    "# 3Ô∏è‚É£ Preparaci√≥n de los Datos para el Modelo\n",
    "X_train = df_train[predictors]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# 4Ô∏è‚É£ Entrenamiento del Modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5Ô∏è‚É£ Evaluaci√≥n del Modelo\n",
    "y_pred_train = model.predict(X_train)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "mape = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100  # Error absoluto porcentual medio\n",
    "\n",
    "print(f\"R¬≤: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")  # Muestra el error en porcentaje\n",
    "\n",
    "# 6Ô∏è‚É£ Predicci√≥n e Imputaci√≥n de Valores Nulos\n",
    "if not df_missing.empty:\n",
    "    X_missing = df_missing[predictors]\n",
    "    df.loc[df[target].isna(), target] = model.predict(X_missing).astype('float32')"
   ],
   "id": "efb9649647157a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üìä Evaluaci√≥n del Modelo de Imputaci√≥n para `floor_area`\n",
    "# - R¬≤ = 0.872 ‚Üí El modelo explica el 87.2% de la variabilidad en `floor_area`, lo que indica un buen ajuste.\n",
    "# - MAE = 9.47 ‚Üí En promedio, el error absoluto en la imputaci√≥n es de 9.47 m¬≤, un error moderado en la escala de la variable.\n",
    "# - RMSE = 26.14 ‚Üí El error cuadr√°tico medio sugiere que existen algunos valores m√°s alejados de la realidad (posiblemente por outliers).\n",
    "# - MAPE = 10.21% ‚Üí En promedio, los valores imputados tienen un error del 10.21% respecto a los valores reales,\n",
    "#   lo que indica una imputaci√≥n razonablemente precisa, aunque con margen de mejora en valores extremos.\n",
    "#\n",
    "# üîπ Conclusi√≥n: La regresi√≥n lineal m√∫ltiple ofrece una buena precisi√≥n en la imputaci√≥n de `floor_area`, con un R¬≤ alto y errores aceptables.\n",
    "#   Sin embargo, los valores m√°s altos de `floor_area` podr√≠an estar generando desviaciones en la predicci√≥n.\n",
    "#   Se recomienda revisar outliers y considerar t√©cnicas m√°s avanzadas (como regresi√≥n robusta o √°rboles de decisi√≥n) para mejorar la estimaci√≥n.\n",
    "\n",
    "# TODO ¬øDeber√≠a tratar outliers aqu√≠?"
   ],
   "id": "9c82ce87fdf43d8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3.5 - floor\n",
   "id": "1e9ec18150469f78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 - Modelado",
   "id": "2887ed7741727105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8fc31a1df3a43384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 - Evaluaci√≥n y conclusiones",
   "id": "7e5d1b29cf1605ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60a067cba282924a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
