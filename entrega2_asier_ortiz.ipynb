{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Mejoras para este ejercicio:\n",
    "# - Asegurar que los datos sean consistentes: si hay valores erróneos o nulos, establecer un random_state para garantizar reproducibilidad\n",
    "# - Implementar pipelines para estructurar mejor el flujo de preprocesamiento y modelado, evitando la aplicación manual de cada paso\n",
    "# - Optimizar el modelo ajustando hiperparámetros con técnicas como GridSearchCV o RandomizedSearchCV\n",
    "# - Utilizar Regex para validaciones: códigos postales, teléfonos, emails, etc.\n",
    "# - Crear variables derivadas como precio por metro cuadrado (precio_m2 = precio / superficie)\n",
    "# - Geolocalización: obtener coordenadas con OpenStreetMap a partir de direcciones o códigos postales y utilizarlas para análisis espaciales\n",
    "# - Visualizar las viviendas en un mapa interactivo con Folium o Plotly Express para identificar patrones geográficos en los precios\n",
    "# - Clusterización de zonas con K-Means o DBSCAN para detectar patrones de precios por ubicación y segmentar mejor los inmuebles\n",
    "# - Evitar data leakage: Dividir los datos en train/test antes de hacer encoding, eliminar outliers o escalar,\n",
    "#   asegurando que las transformaciones se ajusten sólo con el conjunto de entrenamiento y luego se apliquen en test\n",
    "# - Subir el proyecto final a Kaggle"
   ],
   "id": "be40bb5aac5b5e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Funciones Helper**\n",
    "Aquí se encuentran las funciones auxiliares utilizadas en este notebook. Estas funciones permiten realizar tareas específicas de manera más ordenada y modular."
   ],
   "id": "dedb5826bbe4b858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_floor_column(df):\n",
    "    \"\"\"\n",
    "    Limpia la columna 'floor' asegurando consistencia en los valores.\n",
    "    - Convierte valores ordinales como '1st', '2nd', '3rd' en números.\n",
    "    - Sustituye el valor 'ground' por 0.\n",
    "    - Convierte en NaN otros valores como 'floor'.\n",
    "    - Maneja valores extremadamente altos o negativos.\n",
    "    - Convierte a tipo Int16 para optimizar memoria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mostrar valores únicos antes del saneamiento\n",
    "    print(\"\\nValores únicos en 'floor' antes del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    # Diccionario de conversión de valores ordinales (1st, 2nd, etc.)\n",
    "    ordinal_mapping = {f\"{i}th\": i for i in range(1, 101)}\n",
    "    ordinal_mapping.update({\"ground\": 0, \"1st\": 1, \"2nd\": 2, \"3rd\": 3})\n",
    "\n",
    "    # Convertir valores de la columna\n",
    "    cleaned_floors = []\n",
    "    for value in df[\"floor\"]:\n",
    "        if pd.isna(value):\n",
    "            cleaned_floors.append(pd.NA)  # Mantener valores nulos\n",
    "            continue\n",
    "\n",
    "        value = str(value).lower().strip()\n",
    "\n",
    "        # Intentar convertir directamente si es un número\n",
    "        try:\n",
    "            num_value = int(value.replace(\",\", \"\"))  # Manejo de valores con comas como \"1,200\"\n",
    "            if 0 <= num_value <= 100:  # Limitar a un rango razonable de pisos\n",
    "                cleaned_floors.append(num_value)\n",
    "            else:\n",
    "                cleaned_floors.append(pd.NA)  # Si el número es absurdo, lo dejamos como NA\n",
    "        except ValueError:\n",
    "            cleaned_floors.append(ordinal_mapping.get(value, pd.NA))  # Mapear valores ordinales o NA si no se reconoce\n",
    "\n",
    "    # Asignar la columna limpia al DataFrame\n",
    "    df[\"floor\"] = cleaned_floors\n",
    "\n",
    "    # Convertir a Int16 para optimizar memoria, manteniendo NaN con pd.NA\n",
    "    df[\"floor\"] = df[\"floor\"].astype(\"Int16\") # TODO Hacer esto en la conversión\n",
    "\n",
    "    # Mostrar valores únicos después del saneamiento\n",
    "    print(\"\\nValores únicos en 'floor' después del saneamiento:\")\n",
    "    print(df[\"floor\"].unique())\n",
    "\n",
    "    return df"
   ],
   "id": "d3321250ab1bbe27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 - Carga de datos y revisión de la estructura del dataset\n",
    "\n",
    "En este apartado se realiza una **primera exploración del dataset** para comprender su estructura, tipo de variables y posibles relaciones entre ellas. Este paso es fundamental para la correcta preparación de los datos antes del modelado.\n",
    "\n",
    "### **1.1 - Importación de librerías y configuración**\n",
    "Se importan las librerías esenciales para el análisis de datos, la visualización y el modelado, incluyendo **Pandas, NumPy, Matplotlib, Seaborn, SciPy y Scikit-Learn**. Además, se configuran algunos parámetros globales de visualización para mejorar la legibilidad de los gráficos.\n",
    "\n",
    "### **1.2 - Carga del dataset**\n",
    "Se carga el conjunto de datos en un **DataFrame de Pandas** desde un archivo CSV. Se incluye una referencia a la fuente del dataset.\n",
    "\n",
    "### **1.3 - Examinar la estructura del dataset**\n",
    "En esta fase se inspecciona la estructura general del dataset para entender su contenido y formato:\n",
    "\n",
    "- **Visualización de las primeras y últimas filas** para detectar posibles errores en la carga de los datos.\n",
    "- **Información general del dataset** (`df.info()`), que muestra el número de registros, tipos de datos y valores nulos.\n",
    "- **Número total de filas y columnas** (`df.shape`).\n",
    "- **Identificación de columnas numéricas y categóricas**, que ayudará en el preprocesamiento.\n",
    "- **Recuento de valores únicos en variables categóricas**, útil para evaluar su diversidad.\n",
    "- **Visualización de los valores únicos en algunas columnas categóricas clave**, como `type`, `floor`, `orientation`, `district` y `subdistrict`, con el objetivo de analizar su distribución y detectar posibles inconsistencias.\n",
    "- **Revisión del número de valores nulos por columna** (`df.isnull().sum()`), lo que permitirá identificar qué variables requieren imputación o eliminación en la etapa de preprocesamiento.\n",
    "- **Resumen estadístico de las variables numéricas**, para analizar su distribución, detectar valores atípicos y entender la escala de los datos.\n",
    "\n",
    "### **1.4 - Comprobación de relaciones potenciales**\n",
    "Para evaluar la viabilidad del modelado, se analizan las correlaciones entre variables numéricas:\n",
    "\n",
    "- Se genera una **matriz de correlación** (`df.corr()`) para medir la relación entre variables.\n",
    "- Se identifican **las variables con mayor impacto en `price`** basándose en la correlación:\n",
    "  - `floor_built` (0.70)\n",
    "  - `floor_area` (0.72)\n",
    "  - `bedrooms` (0.51)\n",
    "  - `bathrooms` (0.69)\n",
    "- Se confirma la pertinencia de `balcony` como una **variable categórica de clasificación**, revisando la distribución de sus valores."
   ],
   "id": "f207b07e6af43aa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 - Importación de librerías y otras configuraciones",
   "id": "68947ca93b1ee80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import folium\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Configuraciones\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1)"
   ],
   "id": "cae123131f2ff1e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 - Carga del dataset",
   "id": "642fbdc3166ffe7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar el dataset local con Pandas\n",
    "df = pd.read_csv(\"scripts/madrid_rent_with_coordinates.csv\")\n",
    "\n",
    "# Ref. https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data"
   ],
   "id": "57f78ce23e88ed17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 - Examinar la estructura del dataset",
   "id": "dc595c93d116a8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las primeras filas para una vista inicial del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())"
   ],
   "id": "a652c94054a75007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las últimas filas para identificar posibles problemas en la carga de datos\n",
    "print(\"Últimas filas del dataset:\")\n",
    "display(df.tail())"
   ],
   "id": "b29d30b31ad16871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Obtener información general sobre el dataset, incluyendo tipos de datos y valores nulos\n",
    "print(\"Información general del dataset:\")\n",
    "display(df.info())"
   ],
   "id": "cff1b45136ac14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el número de filas y columnas en el dataset\n",
    "print(\"Número de filas y columnas en el dataset:\")\n",
    "print(df.shape)"
   ],
   "id": "d72fee52d985c328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar las columnas numéricas y categóricas\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ],
   "id": "6cc0a5878406bc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas numéricas identificadas\n",
    "print(\"Columnas numéricas:\")\n",
    "print(numerical_columns)"
   ],
   "id": "b8e474b4b602fdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar las columnas categóricas identificadas\n",
    "print(\"Columnas categóricas:\")\n",
    "print(categorical_columns)"
   ],
   "id": "3c8f87e02e5478fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar valores únicos en variables categóricas\n",
    "print(\"Valores únicos en variables categóricas:\")\n",
    "df[categorical_columns].nunique()"
   ],
   "id": "1e7904d2ca243a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores únicos de algunas columnas categóricas para revisar su diversidad\n",
    "relevant_categorical_columns = ['type', 'floor', 'orientation', 'district', 'subdistrict']\n",
    "\n",
    "print(\"Valores únicos en algunas variables categóricas relevantes:\")\n",
    "for col in relevant_categorical_columns:\n",
    "    print(f\"\\n{col}: {df[col].nunique()} valores únicos\")\n",
    "    print(df[col].unique()[:10])"
   ],
   "id": "d0f9ab1f50b72cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar el número de valores nulos por columna\n",
    "print(df.isnull().sum())"
   ],
   "id": "aeed5067783abd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Describir estadísticamente las variables numéricas para analizar su distribución y posibles valores atípicos\n",
    "print(\"Resumen estadístico de las variables numéricas:\")\n",
    "display(df.describe())"
   ],
   "id": "5f6b3ad50b6bc66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 - Comprobar relaciones potenciales",
   "id": "3c939f1df9844c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se genera la matriz de correlación para analizar la relación entre las variables numéricas\n",
    "print(\"Matriz de correlación entre variables numéricas:\")\n",
    "correlation_matrix = df[numerical_columns].corr()\n",
    "display(correlation_matrix)"
   ],
   "id": "b88cd433b80c151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar relaciones relevantes para modelado\n",
    "\n",
    "# Basándonos en la matriz de correlación, seleccionamos las variables con mayor impacto en 'price'\n",
    "# Según el análisis previo, las variables con correlación más fuerte con 'price' son:\n",
    "# - floor_built (0.70)\n",
    "# - floor_area (0.72)\n",
    "# - bedrooms (0.51)\n",
    "# - bathrooms (0.69)\n",
    "# Otras variables tienen correlaciones insignificantes (<0.01) y no se consideran para modelado\n",
    "\n",
    "key_relationships = ['price', 'floor_built', 'floor_area', 'bedrooms', 'bathrooms']\n",
    "print(\"Relaciones clave para la predicción de price:\")\n",
    "display(df[key_relationships].corr())"
   ],
   "id": "7f431580b67734b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se selecciona balcony como variable de clasificación\n",
    "# Se analiza la frecuencia de los valores presentes en balcony para entender su distribución\n",
    "print(\"Frecuencia de valores en la variable balcony:\")\n",
    "df['balcony'].value_counts()"
   ],
   "id": "99008d5a2872faa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 - Limpieza y validación de los datos",
   "id": "5908d74e2a81ac48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 - Eliminación de columnas irrelevantes",
   "id": "ef1f12f8be859629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se eliminan columnas que no aportan información útil para el análisis de precios y balcones\n",
    "columns_to_drop = [\n",
    "    'web_id',               # Identificador único para cada anuncio, no aporta valor analítico\n",
    "    'url',                  # Enlace a la página del anuncio, irrelevante para el modelado\n",
    "    'title',                # Contiene información redundante, ya que 'type' y 'location' están en otras columnas\n",
    "    'professional_name',    # Nombre de la agencia inmobiliaria o propietario, sin impacto en la predicción\n",
    "    'last_update',          # Indica la fecha de última actualización del anuncio, pero sin un formato uniforme y sin valor predictivo\n",
    "    'location'              # Contiene el nombre de la calle y en algunos casos el número, pero ya se tiene 'district' y 'subdistrict'\n",
    "]\n",
    "\n",
    "# Eliminamos las columnas del dataframe\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostramos el nuevo número de columnas para verificar la eliminación\n",
    "print(\"Columnas eliminadas:\", columns_to_drop)\n",
    "print(\"Número de columnas tras la eliminación:\", df.shape[1])"
   ],
   "id": "f033ecda8ee99955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 - Manejo de duplicados",
   "id": "989a9fc4e765552d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Contar filas duplicadas considerando todas las columnas\n",
    "duplicates_total = df.duplicated().sum()\n",
    "print(f\"Número de filas completamente duplicadas: {duplicates_total}\")"
   ],
   "id": "7d37cac7a17b1ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Seleccionar columnas clave para identificar duplicados parciales excluyendo IDs o metadatos\n",
    "#\n",
    "# Para detectar duplicados, se consideran las columnas que describen las características estructurales de la vivienda,\n",
    "# evitando aquellas que son únicas para cada anuncio o irrelevantes para el análisis.\n",
    "#\n",
    "# Columnas **NO INCLUIDAS** en la detección de duplicados y razones:\n",
    "# - `web_id` y `url`: Son identificadores únicos de cada anuncio, por lo que no sirven para detectar duplicados.\n",
    "# - `title`: Contiene información redundante sobre el tipo de vivienda y ubicación, pero en formato texto libre, lo que\n",
    "#   lo hace inconsistente para la comparación.\n",
    "# - `professional_name`: El nombre de la agencia o propietario no influye en si un anuncio es duplicado o no.\n",
    "# - `last_update`: Fecha de actualización del anuncio con valores inconsistentes (por ejemplo, \"2 months\" vs. \"5 November\"),\n",
    "#   lo que impide su uso para identificar duplicados.\n",
    "# - `location`: Contiene el nombre de la calle y en algunos casos el número, pero ya disponemos de `district` y `subdistrict`\n",
    "#   que son más estructurados. Además, las direcciones pueden estar escritas de forma diferente en anuncios duplicados.\n",
    "#\n",
    "# Se opta por considerar características clave de la vivienda como `price`, `bedrooms`, `floor_area`, `year_built`, etc.,\n",
    "# ya que un mismo inmueble puede aparecer varias veces con ligeras variaciones en el título, la inmobiliaria o la fecha de publicación.\n",
    "\n",
    "duplicate_columns = ['type', 'price', 'deposit', 'floor_built', 'floor_area',\n",
    "                     'year_built', 'bedrooms', 'bathrooms', 'second_hand',\n",
    "                     'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                     'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                     'storeroom', 'swimming_pool', 'garden_area', 'district', 'subdistrict',\n",
    "                     'postalcode', 'lat', 'lng']\n",
    "\n",
    "# Contar filas que tienen duplicación en estas columnas\n",
    "duplicates_partial = df.duplicated(subset=duplicate_columns).sum()\n",
    "print(f\"Número de filas duplicadas considerando solo columnas clave: {duplicates_partial}\")"
   ],
   "id": "97db98a7af945d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar filas completamente duplicadas\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataset después de eliminar duplicados completos: {df.shape}\")"
   ],
   "id": "a935b676f343b279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Eliminar duplicados en base a columnas clave, manteniendo la primera aparición\n",
    "df.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)\n",
    "print(f\"Dataset después de eliminar duplicados parciales: {df.shape}\")"
   ],
   "id": "243823915c0f68e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 - Eliminación de columnas con alto porcentaje de nulos",
   "id": "581e0484cc64c14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "\n",
    "# Mostrar el porcentaje de nulos ordenado de mayor a menor\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage.sort_values(ascending=False))"
   ],
   "id": "92eef3b52f84fc9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  floor_area (57%) → Muy relevante para el precio. Mantener y evaluar imputación aunque tenga un alto porcentaje de nulos\n",
    "#  deposit (41%) → No es muy relevante para el precio. Pese a ello, mantener y evaluar imputación aunque tenga un alto porcentaje de nulos\n",
    "\n",
    "# Definir las columnas a eliminar\n",
    "columns_to_drop = [\n",
    "    'year_built',      # Muchos nulos (68%). Aunque debería ser relevante para price (construcciones modernas podrían ser más caras) es difícil de imputar correctamente\n",
    "    'orientation',     # Muchos nulos (52%), difícil de imputar correctamente\n",
    "    'postalcode'       # Porcentaje de nulos moderadamente alto (25%). Redundante con district y subdistrict, no aporta información adicional\n",
    "]\n",
    "\n",
    "# Eliminar las columnas del DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Mostrar las columnas eliminadas y el nuevo número de columnas\n",
    "print(\"Columnas eliminadas por alto porcentaje de valores nulos:\", columns_to_drop)\n",
    "print(\"Número de columnas después de la eliminación:\", df.shape[1])\n"
   ],
   "id": "bab2a59024a96627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 - Saneamiento de valores en columnas categóricas",
   "id": "1315567ee7068831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneización de la columna 'type' para asegurar consistencia en los valores\n",
    "print(\"Valores únicos en 'type' antes del saneamiento:\", df['type'].unique(), \"\\n\")\n",
    "\n",
    "# Elimino espacios en blanco y capitalizo los valores\n",
    "df['type'] = df['type'].str.strip().str.title()\n",
    "\n",
    "print(\"Valores únicos en 'type' después del saneamiento:\", df['type'].unique())"
   ],
   "id": "4447283eec9bb833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Limpieza a la columna 'floor' para estandarizar valores, corregir inconsistencias y pasar los datos a numérico mediante la función 'clear_floor_column'\n",
    "df = clean_floor_column(df)"
   ],
   "id": "dddc934de804cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Homogeneización básica de 'district' y 'subdistrict'\n",
    "\n",
    "# Estas columnas solo se utilizarán en el análisis exploratorio de datos (EDA),\n",
    "# por lo que aplicamos una limpieza mínima para garantizar consistencia:\n",
    "# - Convertimos los valores a minúsculas para evitar diferencias por mayúsculas/minúsculas.\n",
    "# - Eliminamos espacios en blanco adicionales para garantizar uniformidad.\n",
    "# - Reemplazamos valores 'nan' en formato string por valores NaN reales.\n",
    "# - No realizamos agrupaciones de nombres similares, ya que no afectarán al modelo.\n",
    "\n",
    "# Asegurar que sean de tipo 'object' antes de limpiar (evita problemas si son 'category')\n",
    "df['district'] = df['district'].astype(str).str.strip().str.lower()\n",
    "df['subdistrict'] = df['subdistrict'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Reemplazar valores 'nan' string por NaN reales de manera robusta\n",
    "df['district'] = df['district'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "df['subdistrict'] = df['subdistrict'].replace('nan', pd.NA).fillna(pd.NA)\n",
    "\n",
    "# Mostramos los valores únicos después del saneamiento\n",
    "print(\"Valores únicos en 'district' después del saneamiento:\")\n",
    "display(df['district'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Valores únicos en 'subdistrict' después del saneamiento:\")\n",
    "display(df['subdistrict'].unique())"
   ],
   "id": "6e742c95b92a7ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 - Saneamiento de valores en columnas numéricas",
   "id": "dcf7c9b60efd7137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores negativos donde no pueden existir\n",
    "not_neg_columns = ['price', 'deposit', 'floor_area', 'floor_built', 'floor', 'bedrooms', 'bathrooms']\n",
    "for col in not_neg_columns:\n",
    "    df.loc[df[col] < 0, col] = pd.NA"
   ],
   "id": "69b67e1fb540322b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convierto a NaN los valores 0 en columnas donde no tiene sentido\n",
    "not_columns_zero = ['price', 'floor_area', 'floor_built']\n",
    "for col in not_columns_zero:\n",
    "    df.loc[df[col] == 0, col] = pd.NA"
   ],
   "id": "d3e48d4f9ccd58c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar valores negativos restantes\n",
    "for col in not_neg_columns:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    print(f\" {neg_count} valores negativos restantes en '{col}'.\")"
   ],
   "id": "d73d99ac208c5f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar cuántos valores nulos quedaron en cada columna numérica saneada\n",
    "print(\"\\nValores nulos después del saneamiento:\")\n",
    "print(df[not_neg_columns].isna().sum())"
   ],
   "id": "9b8cfd976a1e7767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 - Conversión y optimización de tipos de datos",
   "id": "faa46f73f2e9d826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se ajustan los tipos de datos para reducir el consumo de memoria:\n",
    "# - Columnas enteras: Se asigna el tipo más eficiente según el rango de valores observados.\n",
    "# - Columnas flotantes: Se mantienen en float64 solo las coordenadas, ya que requieren precisión.\n",
    "# - Columnas booleanas: Se convierten a bool para representar valores binarios.\n",
    "# - Columnas categóricas: Se transforman a 'category' para optimizar análisis y almacenamiento.\n",
    "\n",
    "# Columnas enteras\n",
    "df['price'] = df['price'].astype('Int32')\n",
    "df['floor_built'] = df['floor_built'].astype('Int16')\n",
    "df['floor'] = df['floor'].astype('Int8')\n",
    "df['bedrooms'] = df['bedrooms'].astype('Int8')\n",
    "df['bathrooms'] = df['bathrooms'].astype('Int8')\n",
    "df['deposit'] = df['deposit'].astype('Int8')\n",
    "df['floor_area'] = df['floor_area'].astype('Int16')\n",
    "\n",
    "# Columnas flotantes\n",
    "df['lat'] = df['lat'].astype('float64') # Mantengo la precisión para las coordenadas\n",
    "df['lng'] = df['lng'].astype('float64')\n",
    "\n",
    "# Columnas booleanas\n",
    "bool_columns = ['second_hand', 'lift', 'garage_included', 'furnished', 'equipped_kitchen',\n",
    "                'fitted_wardrobes', 'air_conditioning', 'terrace', 'balcony',\n",
    "                'storeroom', 'swimming_pool', 'garden_area', 'private_owner']\n",
    "for col in bool_columns:\n",
    "    df[col] = df[col].astype('bool')\n",
    "\n",
    "# Columnas categóricas\n",
    "categorical_columns = ['type', 'district', 'subdistrict']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Reviso cambios\n",
    "print(\"Tipos de datos después de la conversión:\")\n",
    "print(df.dtypes)"
   ],
   "id": "26ddbb6595f8498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 - Imputación de valores nulos",
   "id": "947bdab76f36b368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.7.1",
   "id": "46ec01bae0dbd0a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filtrar columnas con valores nulos mayores a 0\n",
    "null_values = df.isnull().sum()\n",
    "null_values = null_values[null_values > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Número de valores nulos por columna:\")\n",
    "print(null_values)"
   ],
   "id": "de76c6b3c4c345b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filtrar columnas con porcentaje de nulos mayor a 0\n",
    "null_percentage = df.isnull().mean() * 100\n",
    "null_percentage = null_percentage[null_percentage > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Porcentaje de valores nulos por columna:\")\n",
    "print(null_percentage)"
   ],
   "id": "a63424d193e5c6f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.7.2 - Deposit",
   "id": "ed74a2333a35765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Moda:\", df['deposit'].mode()[0])\n",
    "print(\"Mediana:\", df['deposit'].median())\n",
    "print(\"Media:\", df['deposit'].mean())\n",
    "print(\"Varianza:\", df['deposit'].var())\n",
    "print(\"Desviación estándar:\", df['deposit'].std())\n",
    "print(\"Rango:\", df['deposit'].max() - df['deposit'].min())"
   ],
   "id": "48ab9fc5492ad6b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Frecuencia absoluta\n",
    "print(df['deposit'].value_counts())\n",
    "# Frecuencia en porcentaje\n",
    "print(df['deposit'].value_counts(normalize=True) * 100)"
   ],
   "id": "e52379fcd5558e11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ver correlaciones\n",
    "print(df.select_dtypes(include=['number']).corr()['deposit'].sort_values(ascending=False))"
   ],
   "id": "26e8d13fb64a546f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Percentil 25:\", df['deposit'].quantile(0.25))\n",
    "print(\"Percentil 50 (Mediana):\", df['deposit'].quantile(0.50))\n",
    "print(\"Percentil 75:\", df['deposit'].quantile(0.75))\n",
    "print(\"IQR:\", df['deposit'].quantile(0.75) - df['deposit'].quantile(0.25))"
   ],
   "id": "af06469633c01925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO Para district y subdistrict tal vez podría sacarlo de otras colunmnas, que ya he eliminado... 😂",
   "id": "620b995333d06b03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
